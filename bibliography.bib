@software{__fubar__BsmhmmlfGaussianShading2025,
  title = {Bsmhmmlf/{{Gaussian-Shading}}},
  author = {\_\_FUBAR\_\_},
  date = {2025-07-08T07:46:38Z},
  origdate = {2024-04-10T06:26:08Z},
  url = {https://github.com/bsmhmmlf/Gaussian-Shading},
  urldate = {2025-07-17},
  abstract = {[CVPR 2024] Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models}
}

@article{abdulrahmanNovelHybridDCT2019,
  title = {A Novel Hybrid {{DCT}} and {{DWT}} Based Robust Watermarking Algorithm for Color Images},
  author = {Abdulrahman, Ahmed Khaleel and Ozturk, Serkan},
  date = {2019-06-01},
  journaltitle = {Multimed Tools Appl},
  volume = {78},
  number = {12},
  pages = {17027--17049},
  issn = {1573-7721},
  doi = {10.1007/s11042-018-7085-z},
  url = {https://doi.org/10.1007/s11042-018-7085-z},
  urldate = {2025-04-10},
  abstract = {In this paper, a novel robust color image watermarking method based on Discrete Cosine Transform (DCT) and Discrete Wavelet Transform (DWT) is proposed. In this method, RGB cover image is divided into red, green and blue components. DCT and DWT are applied to each color components. Grayscale watermark image is scrambled by using Arnold transform. DCT is performed to the scrambled watermark image. Transformed watermark image is then divided into equal smaller parts. DCT coefficients of each watermark parts are embedded into four DWT bands of the color components of the cover image. The robustness of the proposed color image watermarking has been demonstrated by applying various image processing operations such as rotating, resizing, filtering, jpeg compression, and noise adding to the watermarked images. Experimental results show that the proposed method is robust to the linear and nonlinear attacks and the transparency of the watermarked images has been protected.},
  langid = {english},
  keywords = {Arnold transform,Color image watermarking,DCT,DWT,RGB}
}

@online{alfarraRobustnessQualityMeasures2022,
  title = {On the {{Robustness}} of {{Quality Measures}} for {{GANs}}},
  author = {Alfarra, Motasem and Pérez, Juan C. and Frühstück, Anna and Torr, Philip H. S. and Wonka, Peter and Ghanem, Bernard},
  date = {2022-07-20},
  eprint = {2201.13019},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2201.13019},
  url = {http://arxiv.org/abs/2201.13019},
  urldate = {2024-11-28},
  abstract = {This work evaluates the robustness of quality measures of generative models such as Inception Score (IS) and Fr\textbackslash 'echet Inception Distance (FID). Analogous to the vulnerability of deep models against a variety of adversarial attacks, we show that such metrics can also be manipulated by additive pixel perturbations. Our experiments indicate that one can generate a distribution of images with very high scores but low perceptual quality. Conversely, one can optimize for small imperceptible perturbations that, when added to real world images, deteriorate their scores. We further extend our evaluation to generative models themselves, including the state of the art network StyleGANv2. We show the vulnerability of both the generative model and the FID against additive perturbations in the latent space. Finally, we show that the FID can be robustified by simply replacing the standard Inception with a robust Inception. We validate the effectiveness of the robustified metric through extensive experiments, showing it is more robust against manipulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning}
}

@online{AndersenStabilityAI,
  title = {Andersen v. {{Stability AI Ltd}}., 3:23-Cv-00201 - {{CourtListener}}.Com},
  shorttitle = {Andersen v. {{Stability AI Ltd}}., 3},
  url = {https://www.courtlistener.com/docket/66732129/andersen-v-stability-ai-ltd/},
  urldate = {2025-07-18},
  abstract = {Docket for Andersen v. Stability AI Ltd., 3:23-cv-00201 — Brought to you by Free Law Project, a non-profit dedicated to creating high quality open legal information.},
  langid = {american},
  organization = {CourtListener}
}

@software{andreasAndmillAwesomeGenAIWatermarking2025,
  title = {And-Mill/{{Awesome-GenAI-Watermarking}}},
  author = {Andreas},
  date = {2025-04-28T16:43:05Z},
  origdate = {2024-03-26T12:19:33Z},
  url = {https://github.com/and-mill/Awesome-GenAI-Watermarking},
  urldate = {2025-04-28},
  abstract = {A curated list of watermarking schemes for generative AI models},
  keywords = {generative-ai,watermark,watermarking}
}

@online{anWAVESBenchmarkingRobustness2024,
  title = {{{WAVES}}: {{Benchmarking}} the {{Robustness}} of {{Image Watermarks}}},
  shorttitle = {{{WAVES}}},
  author = {An, Bang and Ding, Mucong and Rabbani, Tahseen and Agrawal, Aakriti and Xu, Yuancheng and Deng, Chenghao and Zhu, Sicheng and Mohamed, Abdirisak and Wen, Yuxin and Goldstein, Tom and Huang, Furong},
  date = {2024-06-07},
  eprint = {2401.08573},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.08573},
  url = {http://arxiv.org/abs/2401.08573},
  urldate = {2025-07-08},
  abstract = {In the burgeoning age of generative AI, watermarks act as identifiers of provenance and artificial content. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a benchmark for assessing image watermark robustness, overcoming the limitations of current evaluation methods. WAVES integrates detection and identification tasks and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced, novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. Our novel, comprehensive evaluation reveals previously undetected vulnerabilities of several modern watermarking algorithms. We envision WAVES as a toolkit for the future development of robust watermarks. The project is available at https://wavesbench.github.io/},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{arabiSEALSemanticAware2025,
  title = {{{SEAL}}: {{Semantic Aware Image Watermarking}}},
  shorttitle = {{{SEAL}}},
  author = {Arabi, Kasra and Witter, R. Teal and Hegde, Chinmay and Cohen, Niv},
  date = {2025-04-09},
  eprint = {2503.12172},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.12172},
  url = {http://arxiv.org/abs/2503.12172},
  urldate = {2025-07-08},
  abstract = {Generative models have rapidly evolved to generate realistic outputs. However, their synthetic outputs increasingly challenge the clear distinction between natural and AI-generated content, necessitating robust watermarking techniques. Watermarks are typically expected to preserve the integrity of the target image, withstand removal attempts, and prevent unauthorized replication onto unrelated images. To address this need, recent methods embed persistent watermarks into images produced by diffusion models using the initial noise. Yet, to do so, they either distort the distribution of generated images or rely on searching through a long dictionary of used keys for detection. In this paper, we propose a novel watermarking method that embeds semantic information about the generated image directly into the watermark, enabling a distortion-free watermark that can be verified without requiring a database of key patterns. Instead, the key pattern can be inferred from the semantic embedding of the image using locality-sensitive hashing. Furthermore, conditioning the watermark detection on the original image content improves robustness against forgery attacks. To demonstrate that, we consider two largely overlooked attack strategies: (i) an attacker extracting the initial noise and generating a novel image with the same pattern; (ii) an attacker inserting an unrelated (potentially harmful) object into a watermarked image, possibly while preserving the watermark. We empirically validate our method's increased robustness to these attacks. Taken together, our results suggest that content-aware watermarks can mitigate risks arising from image-generative models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{begumDigitalImageWatermarking2020,
  title = {Digital {{Image Watermarking Techniques}}: {{A Review}}},
  shorttitle = {Digital {{Image Watermarking Techniques}}},
  author = {Begum, Mahbuba and Uddin, Mohammad Shorif},
  date = {2020-02},
  journaltitle = {Information},
  volume = {11},
  number = {2},
  pages = {110},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2078-2489},
  doi = {10.3390/info11020110},
  url = {https://www.mdpi.com/2078-2489/11/2/110},
  urldate = {2024-12-03},
  abstract = {Digital image authentication is an extremely significant concern for the digital revolution, as it is easy to tamper with any image. In the last few decades, it has been an urgent concern for researchers to ensure the authenticity of digital images. Based on the desired applications, several suitable watermarking techniques have been developed to mitigate this concern. However, it is tough to achieve a watermarking system that is simultaneously robust and secure. This paper gives details of standard watermarking system frameworks and lists some standard requirements that are used in designing watermarking techniques for several distinct applications. The current trends of digital image watermarking techniques are also reviewed in order to find the state-of-the-art methods and their limitations. Some conventional attacks are discussed, and future research directions are given.},
  issue = {2},
  langid = {english},
  keywords = {DCT,DFT,DWT,LSB,SVD}
}

@article{benderTechniquesDataHiding1996,
  title = {Techniques for Data Hiding},
  author = {Bender, W. and Gruhl, D. and Morimoto, N. and Lu, A.},
  date = {1996},
  journaltitle = {IBM Systems Journal},
  volume = {35},
  number = {3.4},
  pages = {313--336},
  issn = {0018-8670},
  doi = {10.1147/sj.353.0313},
  url = {https://ieeexplore.ieee.org/document/5387237},
  urldate = {2024-12-03},
  abstract = {Data hiding, a form of steganography, embeds data into digital media for the purpose of identification, annotation, and copyright. Several constraints affect this process: the quantity of data to be hidden, the need for invariance of these data under conditions where a "host" signal is subject to distortions, e.g., lossy compression, and the degree to which the data must be immune to interception, modification, or removal by a third party. We explore both traditional and novel techniques for addressing the data-hiding process and evaluate these techniques in light of three applications: copyright protection, tamper-proofing, and augmentation data embedding.},
  eventtitle = {{{IBM Systems Journal}}}
}

@online{BingPreviewRelease2023,
  title = {Bing {{Preview Release Notes}}: {{New}} Experiences Powered by {{Bing Image Creator}}},
  shorttitle = {Bing {{Preview Release Notes}}},
  date = {2023-09-29},
  url = {https://blogs.bing.com/search/september-2023/Bing-Preview-Release-Notes-New-Experiences-Powered-by-Bing-Image-Creator},
  urldate = {2024-11-28},
  abstract = {Last week was a big one! We announced lots of new experiences coming to Bing Chat\&mdash;including personalized answers, DALL-E 3, and new AI-powered features in SwiftKey. Over the next few weeks, you\&rsquo;ll see some of these features appear as we test them before release. These Release Notes will continue to update you on which features are fully available to all users.\&nbsp;},
  langid = {american}
}

@online{birhaneMultimodalDatasetsMisogyny2021,
  title = {Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes},
  shorttitle = {Multimodal Datasets},
  author = {Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel},
  date = {2021-10-05},
  eprint = {2110.01963},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2110.01963},
  url = {http://arxiv.org/abs/2110.01963},
  urldate = {2024-11-15},
  abstract = {We have now entered the era of trillion parameter machine learning models trained on billion-sized datasets scraped from the internet. The rise of these gargantuan datasets has given rise to formidable bodies of critical work that has called for caution while generating these large datasets. These address concerns surrounding the dubious curation practices used to generate these datasets, the sordid quality of alt-text data available on the world wide web, the problematic content of the CommonCrawl dataset often used as a source for training large language models, and the entrenched biases in large-scale visio-linguistic models (such as OpenAI's CLIP model) trained on opaque datasets (WebImageText). In the backdrop of these specific calls of caution, we examine the recently released LAION-400M dataset, which is a CLIP-filtered dataset of Image-Alt-text pairs parsed from the Common-Crawl dataset. We found that the dataset contains, troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content. We outline numerous implications, concerns and downstream harms regarding the current state of large scale datasets while raising open questions for various stakeholders including the AI community, regulators, policy makers and data subjects.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society}
}

@online{bohacekNepotisticallyTrainedGenerativeAI2023,
  title = {Nepotistically {{Trained Generative-AI Models Collapse}}},
  author = {Bohacek, Matyas and Farid, Hany},
  date = {2023-11-20},
  eprint = {2311.12202},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2311.12202},
  url = {http://arxiv.org/abs/2311.12202},
  urldate = {2024-11-29},
  abstract = {Trained on massive amounts of human-generated content, AI (artificial intelligence) image synthesis is capable of reproducing semantically coherent images that match the visual appearance of its training data. We show that when retrained on even small amounts of their own creation, these generative-AI models produce highly distorted images. We also show that this distortion extends beyond the text prompts used in retraining, and that once poisoned, the models struggle to fully heal even after retraining on only real images.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@book{borsImageWatermarkingUsing1996,
  title = {Image Watermarking Using {{DCT}} Domain Constraints},
  author = {Bors, Adrian and Pitas, I.},
  date = {1996-09-16},
  pages = {234 vol.3},
  doi = {10.1109/ICIP.1996.560426},
  abstract = {Watermarking algorithms are used for image copyright protection. The algorithms proposed select certain blocks in the image based on a Gaussian network classifier. The pixel values of the selected blocks are modified such that their discrete cosine transform (DCT) coefficients fulfil a constraint imposed by the watermark code. Two different constraints are considered. The first approach consists of embedding a linear constraint among selected DCT coefficients and the second one defines circular detection regions in the DCT domain. A rule for generating the DCT parameters of distinct watermarks is provided. The watermarks embedded by the proposed algorithms are resistant to JPEG compression},
  isbn = {978-0-7803-3259-1},
  pagetotal = {231}
}

@online{buiRoSteALSRobustSteganography2023,
  title = {{{RoSteALS}}: {{Robust Steganography}} Using {{Autoencoder Latent Space}}},
  shorttitle = {{{RoSteALS}}},
  author = {Bui, Tu and Agarwal, Shruti and Yu, Ning and Collomosse, John},
  date = {2023-04-06},
  eprint = {2304.03400},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.03400},
  url = {http://arxiv.org/abs/2304.03400},
  urldate = {2025-02-15},
  abstract = {Data hiding such as steganography and invisible watermarking has important applications in copyright protection, privacy-preserved communication and content provenance. Existing works often fall short in either preserving image quality, or robustness against perturbations or are too complex to train. We propose RoSteALS, a practical steganography technique leveraging frozen pretrained autoencoders to free the payload embedding from learning the distribution of cover images. RoSteALS has a light-weight secret encoder of just 300k parameters, is easy to train, has perfect secret recovery performance and comparable image quality on three benchmarks. Additionally, RoSteALS can be adapted for novel cover-less steganography applications in which the cover image can be sampled from noise or conditioned on text prompts via a denoising diffusion process. Our model and code are available at \textbackslash url\{https://github.com/TuBui/RoSteALS\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{caoRobustReversibleColor2022,
  title = {Robust and Reversible Color Image Watermarking Based on {{DFT}} in the Spatial Domain},
  author = {Cao, Hongjiao and Hu, Fangxu and Sun, Yehan and Chen, Siyu and Su, Qingtang},
  date = {2022-07-01},
  journaltitle = {Optik},
  volume = {262},
  pages = {169319},
  issn = {0030-4026},
  doi = {10.1016/j.ijleo.2022.169319},
  url = {https://www.sciencedirect.com/science/article/pii/S0030402622006519},
  urldate = {2024-12-07},
  abstract = {With the purpose of protecting the copyright and integrity of color images, a robust and reversible color image watermarking algorithm in the spatial domain fusing discrete Fourier transform (DFT) is proposed. Based on the relationship between the direct current (DC) component of DFT and the spatial-domain pixel values, the proposed quantization technology is used to embed and extract the watermark in this paper. Firstly, color images are represented in RGB color space, and the red, green, and blue channels of the host image are preprocessed for obtaining image blocks of size 2~×~2 and that of the watermark image are preprocessed for obtaining watermark bit sequences. Secondly, the DC component of DFT in each block is calculated and quantized, and the change of the DC component is assigned to each pixel value by the derived formula. Finally, it is completed to embed a 1-bit watermark into one block in the spatial domain. Meanwhile, a matrix that records the magnitude of the change of pixel value in each block is generated to recover the host image, and the related watermark extraction algorithm is also proposed. The experimental results show that this algorithm can completely extract the watermark and restore the host image without loss under the condition of not being attacked, and it has good invisibility, robustness, security, and embedding payload, particularly its running efficiency is also high.},
  keywords = {Color image,DFT,Robust reversible watermarking,Spatial domain}
}

@online{ciWMAdapterAddingWaterMark2024,
  title = {{{WMAdapter}}: {{Adding WaterMark Control}} to {{Latent Diffusion Models}}},
  shorttitle = {{{WMAdapter}}},
  author = {Ci, Hai and Song, Yiren and Yang, Pei and Xie, Jinheng and Shou, Mike Zheng},
  date = {2024-06-12},
  eprint = {2406.08337},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.08337},
  url = {http://arxiv.org/abs/2406.08337},
  urldate = {2024-12-10},
  abstract = {Watermarking is crucial for protecting the copyright of AI-generated images. We propose WMAdapter, a diffusion model watermark plugin that takes user-specified watermark information and allows for seamless watermark imprinting during the diffusion generation process. WMAdapter is efficient and robust, with a strong emphasis on high generation quality. To achieve this, we make two key designs: (1) We develop a contextual adapter structure that is lightweight and enables effective knowledge transfer from heavily pretrained post-hoc watermarking models. (2) We introduce an extra finetuning step and design a hybrid finetuning strategy to further improve image quality and eliminate tiny artifacts. Empirical results demonstrate that WMAdapter offers strong flexibility, exceptional image generation quality and competitive watermark robustness.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing}
}

@software{CompVisStablediffusion2024,
  title = {{{CompVis}}/Stable-Diffusion},
  date = {2024-11-28T17:17:37Z},
  origdate = {2022-08-10T14:36:44Z},
  url = {https://github.com/CompVis/stable-diffusion},
  urldate = {2024-11-28},
  abstract = {A latent text-to-image diffusion model},
  organization = {CompVis - Computer Vision and Learning LMU Munich}
}

@online{ContentCredentialsC2PA,
  title = {Content {{Credentials}} : {{C2PA Technical Specification}} :: {{C2PA Specifications}}},
  url = {https://c2pa.org/specifications/specifications/2.0/specs/C2PA_Specification.html},
  urldate = {2024-12-09}
}

@online{CopyrightRegistrationGuidance2023,
  title = {Copyright {{Registration Guidance}}: {{Works Containing Material Generated}} by {{Artificial Intelligence}}},
  shorttitle = {Copyright {{Registration Guidance}}},
  date = {2023-03-16},
  url = {https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence},
  urldate = {2024-11-28},
  abstract = {The Copyright Office issues this statement of policy to clarify its practices for examining and registering works that contain material generated by the use of artificial intelligence technology.},
  langid = {english},
  organization = {Federal Register}
}

@book{coxDigitalWatermarking2001,
  title = {Digital {{Watermarking}}},
  author = {Cox, I. and Miller, M. and Bloom, J. and Miller, M.},
  date = {2001},
  series = {The {{Morgan Kaufmann Series}} in {{Multimedia Information}} and {{Systems}}},
  publisher = {Morgan Kaufmann},
  url = {https://books.google.co.uk/books?id=uJcEWRv-RRUC},
  isbn = {978-0-08-050459-9}
}

@online{cuiDiffusionShieldWatermarkCopyright2024,
  title = {{{DiffusionShield}}: {{A Watermark}} for {{Copyright Protection}} against {{Generative Diffusion Models}}},
  shorttitle = {{{DiffusionShield}}},
  author = {Cui, Yingqian and Ren, Jie and Xu, Han and He, Pengfei and Liu, Hui and Sun, Lichao and Xing, Yue and Tang, Jiliang},
  date = {2024-05-10},
  eprint = {2306.04642},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.04642},
  url = {http://arxiv.org/abs/2306.04642},
  urldate = {2024-12-10},
  abstract = {Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, DiffusionShield ensures low distortion of the original image, high watermark detection performance, and the ability to embed lengthy messages. We conduct rigorous and comprehensive experiments to show the effectiveness of DiffusionShield in defending against infringement by GDMs and its superiority over traditional watermarking methods. The code for DiffusionShield is accessible in https://github.com/Yingqiancui/DiffusionShield.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  date = {2021-06-01},
  eprint = {2105.05233},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2105.05233},
  url = {http://arxiv.org/abs/2105.05233},
  urldate = {2025-05-10},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\$\textbackslash times\$128, 4.59 on ImageNet 256\$\textbackslash times\$256, and 7.72 on ImageNet 512\$\textbackslash times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\$\textbackslash times\$256 and 3.85 on ImageNet 512\$\textbackslash times\$512. We release our code at https://github.com/openai/guided-diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{DigitalImageWatermarking,
  title = {Digital Image Watermarking: An Overview | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/779111},
  urldate = {2024-12-04}
}

@online{DigitalWatermarkingSteganography,
  title = {Digital {{Watermarking}} and {{Steganography}} | {{Request PDF}}},
  doi = {10.1016/B978-0-12-372585-1.X5001-3},
  url = {https://www.researchgate.net/publication/263606329_Digital_Watermarking_and_Steganography},
  urldate = {2024-11-18},
  abstract = {Request PDF | Digital Watermarking and Steganography | Digital audio, video, images, and documents are flying through cyberspace to their respective owners. Unfortunately, along the way, individuals... | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  organization = {ResearchGate}
}

@online{EfficientSpatialImage,
  title = {Efficient Spatial Image Watermarking via New Perceptual Masking and Blind Detection Schemes | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/1634366},
  urldate = {2024-12-03}
}

@inproceedings{elbasiRobustMedicalImage2018,
  title = {Robust {{Medical Image Watermarking Using Frequency Domain}} and {{Least Significant Bits Algorithms}}},
  booktitle = {2018 {{International Conference}} on {{Computing Sciences}} and {{Engineering}} ({{ICCSE}})},
  author = {Elbasi, Ersin and Kaya, Volkan},
  date = {2018-03},
  pages = {1--5},
  doi = {10.1109/ICCSE1.2018.8374221},
  url = {https://ieeexplore.ieee.org/abstract/document/8374221},
  urldate = {2024-12-03},
  abstract = {Watermarking and stenography are getting importance recently because of copyright protection and authentication. In watermarking we embed stamp, logo, noise or image to multimedia elements such as image, video, audio, animation, software and text. There are several works have been done in watermarking for different purposes. In this research work we used watermarking techniques to embed patient information into the medical magnetic resonance (MR) images. There are two methods have been used; frequency domain (Digital Wavelet Transform-DWT, Digital Cosine Transform-DCT and Digital Fourier Transform-DFT) and spatial domain (Least Significant Bits-LSB). Experimental results show that embedding in frequency domains resist against one group of attacks, and embedding in spatial domain is resist against another group of attacks. Peak Signal Noise Ratio (PSNR) and Similarity Ratio (SR) values are two measurement values for testing. This two values gives very promising result for information hiding in medical MR images.},
  eventtitle = {2018 {{International Conference}} on {{Computing Sciences}} and {{Engineering}} ({{ICCSE}})},
  keywords = {Biomedical imaging,Discrete cosine transforms,Discrete Fourier transforms,Discrete wavelet transforms,Multimedia communication,Watermarking}
}

@article{evsutinWatermarkingSchemesDigital2022,
  title = {Watermarking Schemes for Digital Images: {{Robustness}} Overview},
  shorttitle = {Watermarking Schemes for Digital Images},
  author = {Evsutin, Oleg and Dzhanashia, Kristina},
  date = {2022-01-01},
  journaltitle = {Signal Processing: Image Communication},
  volume = {100},
  pages = {116523},
  issn = {0923-5965},
  doi = {10.1016/j.image.2021.116523},
  url = {https://www.sciencedirect.com/science/article/pii/S0923596521002551},
  urldate = {2024-12-10},
  abstract = {Digital watermarking is an important scientific direction located at the intersection of cybersecurity and multimedia processing. Digital watermarking is used for digital objects copyright protection and protection against forgery. The importance of those tasks is highlighted by the ongoing COVID-19 pandemic that lasts for more than a year and forced multiple industries to transit to online. The main digital watermarking application include active attackers whose goal is to destroy or damage watermarks. Thus, digital watermarking schemes must be robust to attacks. Despite of the large number of existing digital watermarking schemes, a comprehensive analysis of their robustness towards various attacks does not exist. Our study aims to fill this gap. In this article, latest years findings in digital watermarking are systematized. First of all, we review the most prominent distinctive features of watermarking schemes, attack types, and performance measures to facilitate navigation in digital image watermarking. Then, we introduce classification of digital watermarking schemes by their robustness towards various classes of attacks. We believe this classification could be useful for researchers who aim to choose a digital watermarking scheme for their application with a known set of attacks. Finally, we highlight the most promising solutions that further development could lead to schemes with better robustness compared to existing schemes.},
  keywords = {Data hiding,Digital images,Digital watermarking,Information security,Robustness}
}

@article{fazliRobustImageWatermarking2016,
  title = {A Robust Image Watermarking Method Based on {{DWT}}, {{DCT}}, and {{SVD}} Using a New Technique for Correction of Main Geometric Attacks},
  author = {Fazli, Saeid and Moeini, Masoumeh},
  date = {2016},
  journaltitle = {Optik},
  volume = {127},
  number = {2},
  pages = {964--972},
  issn = {0030-4026},
  doi = {10.1016/j.ijleo.2015.09.205},
  url = {https://www.sciencedirect.com/science/article/pii/S0030402615012863},
  abstract = {In this article, a new scheme based on a combination of DWT, DCT, and SVD domains is presented, in which the main focus is to provide proper solutions for reducing effect of geometric attacks. To address this goal, we divide host image into four nonoverlapping rectangular segments called sub-images and then watermark is independently embedded into each of them, using the hybrid scheme. The redundancy reduces effect of cropping attack. Moreover, in order to correct main geometric attacks, such as rotation, translation, and affine translation, we propose an inventional synchronization technique to recover geometrically attacked image via detection of desired image corners. A binary image in the first experiment and some 1D binary random sequences with different lengths in the next experiments are used as watermarks. Since the studied binary sequences have generally smaller lengths than the capacity offered by the proposed scheme, we are motivated to utilize error correction techniques, such as data replication and hamming code for them. Achieved results, compared with other geometric robust schemes, shows that the proposed scheme has stronger or comparable robustness against common signal processing and geometric attacks.},
  keywords = {Discrete cosine transform (DCT),Discrete wavelet transform (DWT),Geometric robust,Image watermarking,Singular value decomposition (SVD)}
}

@online{fengAquaLoRAWhiteboxProtection2024,
  title = {{{AquaLoRA}}: {{Toward White-box Protection}} for {{Customized Stable Diffusion Models}} via {{Watermark LoRA}}},
  shorttitle = {{{AquaLoRA}}},
  author = {Feng, Weitao and Zhou, Wenbo and He, Jiyan and Zhang, Jie and Wei, Tianyi and Li, Guanlin and Zhang, Tianwei and Zhang, Weiming and Yu, Nenghai},
  date = {2024-05-18},
  eprint = {2405.11135},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.11135},
  url = {http://arxiv.org/abs/2405.11135},
  urldate = {2024-11-28},
  abstract = {Diffusion models have achieved remarkable success in generating high-quality images. Recently, the open-source models represented by Stable Diffusion (SD) are thriving and are accessible for customization, giving rise to a vibrant community of creators and enthusiasts. However, the widespread availability of customized SD models has led to copyright concerns, like unauthorized model distribution and unconsented commercial use. To address it, recent works aim to let SD models output watermarked content for post-hoc forensics. Unfortunately, none of them can achieve the challenging white-box protection, wherein the malicious user can easily remove or replace the watermarking module to fail the subsequent verification. For this, we propose \textbackslash texttt\{\textbackslash method\} as the first implementation under this scenario. Briefly, we merge watermark information into the U-Net of Stable Diffusion Models via a watermark Low-Rank Adaptation (LoRA) module in a two-stage manner. For watermark LoRA module, we devise a scaling matrix to achieve flexible message updates without retraining. To guarantee fidelity, we design Prior Preserving Fine-Tuning (PPFT) to ensure watermark learning with minimal impacts on model distribution, validated by proofs. Finally, we conduct extensive experiments and ablation studies to verify our design.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security}
}

@online{fernandezStableSignatureRooting2023,
  title = {The {{Stable Signature}}: {{Rooting Watermarks}} in {{Latent Diffusion Models}}},
  shorttitle = {The {{Stable Signature}}},
  author = {Fernandez, Pierre and Couairon, Guillaume and Jégou, Hervé and Douze, Matthijs and Furon, Teddy},
  date = {2023-07-26},
  eprint = {2303.15435},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.15435},
  url = {http://arxiv.org/abs/2303.15435},
  urldate = {2024-11-18},
  abstract = {Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. This paper introduces an active strategy combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that Stable Signature works even after the images are modified. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep \$10\textbackslash\%\$ of the content, with \$90\$+\$\textbackslash\%\$ accuracy at a false positive rate below 10\$\textasciicircum\{-6\}\$.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{fernandezStableSignatureRooting2023a,
  title = {The {{Stable Signature}}: {{Rooting Watermarks}} in {{Latent Diffusion Models}}},
  shorttitle = {The {{Stable Signature}}},
  author = {Fernandez, Pierre and Couairon, Guillaume and Jégou, Hervé and Douze, Matthijs and Furon, Teddy},
  date = {2023},
  pages = {22466--22477},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Fernandez_The_Stable_Signature_Rooting_Watermarks_in_Latent_Diffusion_Models_ICCV_2023_paper.html},
  urldate = {2025-02-15},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english}
}

@online{fernandezStableSignatureRooting2023b,
  title = {The {{Stable Signature}}: {{Rooting Watermarks}} in {{Latent Diffusion Models}}},
  shorttitle = {The {{Stable Signature}}},
  author = {Fernandez, Pierre and Couairon, Guillaume and Jégou, Hervé and Douze, Matthijs and Furon, Teddy},
  date = {2023-07-26},
  eprint = {2303.15435},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.15435},
  url = {http://arxiv.org/abs/2303.15435},
  urldate = {2025-02-27},
  abstract = {Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. This paper introduces an active strategy combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that Stable Signature works even after the images are modified. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep \$10\textbackslash\%\$ of the content, with \$90\$+\$\textbackslash\%\$ accuracy at a false positive rate below 10\$\textasciicircum\{-6\}\$.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@online{fernandezWatermarkingModalitiesContent2025,
  title = {Watermarking across {{Modalities}} for {{Content Tracing}} and {{Generative AI}}},
  author = {Fernandez, Pierre},
  date = {2025-02-04},
  eprint = {2502.05215},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.05215},
  url = {http://arxiv.org/abs/2502.05215},
  urldate = {2025-02-15},
  abstract = {Watermarking embeds information into digital content like images, audio, or text, imperceptible to humans but robustly detectable by specific algorithms. This technology has important applications in many challenges of the industry such as content moderation, tracing AI-generated content, and monitoring the usage of AI models. The contributions of this thesis include the development of new watermarking techniques for images, audio, and text. We first introduce methods for active moderation of images on social platforms. We then develop specific techniques for AI-generated content. We specifically demonstrate methods to adapt latent generative models to embed watermarks in all generated content, identify watermarked sections in speech, and improve watermarking in large language models with tests that ensure low false positive rates. Furthermore, we explore the use of digital watermarking to detect model misuse, including the detection of watermarks in language models fine-tuned on watermarked text, and introduce training-free watermarks for the weights of large transformers. Through these contributions, the thesis provides effective solutions for the challenges posed by the increasing use of generative AI models and the need for model monitoring and content moderation. It finally examines the challenges and limitations of watermarking techniques and discuss potential future directions for research in this area.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{ferraraGenAIHumanityNefarious2024,
  title = {{{GenAI Against Humanity}}: {{Nefarious Applications}} of {{Generative Artificial Intelligence}} and {{Large Language Models}}},
  shorttitle = {{{GenAI Against Humanity}}},
  author = {Ferrara, Emilio},
  date = {2024-04},
  journaltitle = {J Comput Soc Sc},
  volume = {7},
  number = {1},
  eprint = {2310.00737},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {549--569},
  issn = {2432-2717, 2432-2725},
  doi = {10.1007/s42001-024-00250-1},
  url = {http://arxiv.org/abs/2310.00737},
  urldate = {2024-12-09},
  abstract = {Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI's nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction}
}

@online{goodfellowExplainingHarnessingAdversarial2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  date = {2015-03-20},
  eprint = {1412.6572},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1412.6572},
  url = {http://arxiv.org/abs/1412.6572},
  urldate = {2024-11-15},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2024-11-16},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{guoAIgeneratedImageDetection2024,
  title = {{{AI-generated Image Detection}}: {{Passive}} or {{Watermark}}?},
  shorttitle = {{{AI-generated Image Detection}}},
  author = {Guo, Moyang and Hu, Yuepeng and Jiang, Zhengyuan and Li, Zeyu and Sadovnik, Amir and Daw, Arka and Gong, Neil},
  date = {2024-11-20},
  eprint = {2411.13553},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.13553},
  url = {http://arxiv.org/abs/2411.13553},
  urldate = {2024-12-10},
  abstract = {While text-to-image models offer numerous benefits, they also pose significant societal risks. Detecting AI-generated images is crucial for mitigating these risks. Detection methods can be broadly categorized into passive and watermark-based approaches: passive detectors rely on artifacts present in AI-generated images, whereas watermark-based detectors proactively embed watermarks into such images. A key question is which type of detector performs better in terms of effectiveness, robustness, and efficiency. However, the current literature lacks a comprehensive understanding of this issue. In this work, we aim to bridge that gap by developing ImageDetectBench, the first comprehensive benchmark to compare the effectiveness, robustness, and efficiency of passive and watermark-based detectors. Our benchmark includes four datasets, each containing a mix of AI-generated and non-AI-generated images. We evaluate five passive detectors and four watermark-based detectors against eight types of common perturbations and three types of adversarial perturbations. Our benchmark results reveal several interesting findings. For instance, watermark-based detectors consistently outperform passive detectors, both in the presence and absence of perturbations. Based on these insights, we provide recommendations for detecting AI-generated images, e.g., when both types of detectors are applicable, watermark-based detectors should be the preferred choice.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{guoFreqMarkInvisibleImage2024,
  title = {{{FreqMark}}: {{Invisible Image Watermarking}} via {{Frequency Based Optimization}} in {{Latent Space}}},
  shorttitle = {{{FreqMark}}},
  author = {Guo, Yiyang and Li, Ruizhe and Hui, Mude and Guo, Hanzhong and Zhang, Chen and Cai, Chuangjian and Wan, Le and Wang, Shangfei},
  date = {2024-10-28},
  eprint = {2410.20824},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.20824},
  url = {http://arxiv.org/abs/2410.20824},
  urldate = {2024-11-28},
  abstract = {Invisible watermarking is essential for safeguarding digital content, enabling copyright protection and content authentication. However, existing watermarking methods fall short in robustness against regeneration attacks. In this paper, we propose a novel method called FreqMark that involves unconstrained optimization of the image latent frequency space obtained after VAE encoding. Specifically, FreqMark embeds the watermark by optimizing the latent frequency space of the images and then extracts the watermark through a pre-trained image encoder. This optimization allows a flexible trade-off between image quality with watermark robustness and effectively resists regeneration attacks. Experimental results demonstrate that FreqMark offers significant advantages in image quality and robustness, permits flexible selection of the encoding bit number, and achieves a bit accuracy exceeding 90\% when encoding a 48-bit hidden message under various attack scenarios.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{GustavostaStableDiffusionPromptsDatasets2023,
  title = {Gustavosta/{{Stable-Diffusion-Prompts}} · {{Datasets}} at {{Hugging Face}}},
  date = {2023-03-16},
  url = {https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts},
  urldate = {2025-07-17},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.}
}

@article{haoRobustImageWatermarking2020,
  title = {Robust Image Watermarking Based on Generative Adversarial Network},
  author = {Hao, Kangli and Feng, Guorui and Zhang, Xinpeng},
  date = {2020-11},
  journaltitle = {China Communications},
  volume = {17},
  number = {11},
  pages = {131--140},
  issn = {1673-5447},
  doi = {10.23919/JCC.2020.11.012},
  url = {https://ieeexplore.ieee.org/abstract/document/9267803},
  urldate = {2024-12-03},
  abstract = {Digital watermark embeds information bits into digital cover such as images and videos to prove the creator's ownership of his work. In this paper, we propose a robust image watermark algorithm based on a generative adversarial network. This model includes two modules, generator and adversary. Generator is mainly used to generate images embedded with watermark, and decode the image damaged by noise to obtain the watermark. Adversary is used to discriminate whether the image is embedded with watermark and damage the image by noise. Based on the model Hidden (hiding data with deep networks), we add a high-pass filter in front of the discriminator, making the watermark tend to be embedded in the mid-frequency region of the image. Since the human visual system pays more attention to the central area of the image, we give a higher weight to the image center region, and a lower weight to the edge region when calculating the loss between cover and embedded image. The watermarked image obtained by this scheme has a better visual performance. Experimental results show that the proposed architecture is more robust against noise interference compared with the state-of-art schemes.},
  eventtitle = {China {{Communications}}},
  keywords = {convolutional neural network,Decoding,deep learning,generative adversarial network,Generative adversarial networks,Generators,robust image watermark,Shape,Signal processing algorithms,Transforms,Watermarking}
}

@online{heuselGANsTrainedTwo2018,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2018-01-12},
  eprint = {1706.08500},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.08500},
  url = {http://arxiv.org/abs/1706.08500},
  urldate = {2025-04-15},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2024-11-16},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{houseExecutiveOrderSafe2023,
  title = {Executive {{Order}} on the {{Safe}}, {{Secure}}, and {{Trustworthy Development}} and {{Use}} of {{Artificial Intelligence}}},
  author = {House, The White},
  date = {2023-10-30T20:39:11+00:00},
  url = {https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/},
  urldate = {2024-11-28},
  abstract = {By the authority vested in me as President by the Constitution and the laws of the United States of America, it is hereby ordered as},
  langid = {american},
  organization = {The White House}
}

@online{IdentifyingAIgeneratedImages2024,
  title = {Identifying {{AI-generated}} Images with {{SynthID}}},
  date = {2024-10-30},
  url = {https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/},
  urldate = {2024-11-14},
  abstract = {Today, in partnership with Google Cloud, we're beta launching SynthID, a new tool for watermarking and identifying AI-generated images. It's being released to a limited number of Vertex AI...},
  langid = {english},
  organization = {Google DeepMind}
}

@online{jiangCertifiablyRobustImage2024,
  title = {Certifiably {{Robust Image Watermark}}},
  author = {Jiang, Zhengyuan and Guo, Moyang and Hu, Yuepeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
  date = {2024-07-04},
  eprint = {2407.04086},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2407.04086},
  url = {http://arxiv.org/abs/2407.04086},
  urldate = {2024-11-28},
  abstract = {Generative AI raises many societal concerns such as boosting disinformation and propaganda campaigns. Watermarking AI-generated content is a key technology to address these concerns and has been widely deployed in industry. However, watermarking is vulnerable to removal attacks and forgery attacks. In this work, we propose the first image watermarks with certified robustness guarantees against removal and forgery attacks. Our method leverages randomized smoothing, a popular technique to build certifiably robust classifiers and regression models. Our major technical contributions include extending randomized smoothing to watermarking by considering its unique characteristics, deriving the certified robustness guarantees, and designing algorithms to estimate them. Moreover, we extensively evaluate our image watermarks in terms of both certified and empirical robustness. Our code is available at \textbackslash url\{https://github.com/zhengyuan-jiang/Watermark-Library\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{jiangWatermarkbasedDetectionAttribution2024,
  title = {Watermark-Based {{Detection}} and {{Attribution}} of {{AI-Generated Content}}},
  author = {Jiang, Zhengyuan and Guo, Moyang and Hu, Yuepeng and Gong, Neil Zhenqiang},
  date = {2024-04-05},
  eprint = {2404.04254},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2404.04254},
  url = {http://arxiv.org/abs/2404.04254},
  urldate = {2024-11-14},
  abstract = {Several companies--such as Google, Microsoft, and OpenAI--have deployed techniques to watermark AI-generated content to enable proactive detection. However, existing literature mainly focuses on user-agnostic detection. Attribution aims to further trace back the user of a generative-AI service who generated a given content detected as AI-generated. Despite its growing importance, attribution is largely unexplored. In this work, we aim to bridge this gap by providing the first systematic study on watermark-based, user-aware detection and attribution of AI-generated content. Specifically, we theoretically study the detection and attribution performance via rigorous probabilistic analysis. Moreover, we develop an efficient algorithm to select watermarks for the users to enhance attribution performance. Both our theoretical and empirical results show that watermark-based detection and attribution inherit the accuracy and (non-)robustness properties of the watermarking method.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{kadianRobustDigitalWatermarking2021,
  title = {Robust {{Digital Watermarking Techniques}} for {{Copyright Protection}} of {{Digital Data}}: {{A Survey}}},
  shorttitle = {Robust {{Digital Watermarking Techniques}} for {{Copyright Protection}} of {{Digital Data}}},
  author = {Kadian, Poonam and Arora, Shiafali M. and Arora, Nidhi},
  date = {2021-06-01},
  journaltitle = {Wireless Pers Commun},
  volume = {118},
  number = {4},
  pages = {3225--3249},
  issn = {1572-834X},
  doi = {10.1007/s11277-021-08177-w},
  url = {https://doi.org/10.1007/s11277-021-08177-w},
  urldate = {2024-12-10},
  abstract = {Digital watermarking has emerged as a potential solution to the copyright protection related issues of digital data. This paper presents key paradigms of research in robust watermarking techniques for copyright protection, copy protection, and authentication of digital multimedia. The main issues that derive research in robust watermarking schemes are imperceptibility, security, and robustness. The purpose of this paper is to provide a gist of robust watermarking schemes in the transform domain with the help of some brief theories proposed in the literature. Frequency transformation techniques such as DCT, DFT, and DWT, RDWT have been the most widely used methods to develop robust watermarking algorithms in the transform domain. The general framework of the watermarking system, recent application areas, characteristics, classification of information hiding methods, and various performance evaluation parameters considered by researchers have also been presented in this review. Broadly, this study reviews and compare performance summary of the several state-of-art robust watermarking methods available. This survey paper will be beneficial for the researchers keen to contribute to the field of robust digital watermarking particularly in the transform domain.},
  langid = {english},
  keywords = {Authentication,Copyright protection,Data hiding,Digital watermarking,DWT,Multimedia security,RDWT,Robust watermarking,SVD}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2024-11-16},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{kurakinAdversarialExamplesPhysical2017,
  title = {Adversarial Examples in the Physical World},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  date = {2017-02-11},
  eprint = {1607.02533},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1607.02533},
  urldate = {2024-11-15},
  abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2015-02-21},
  eprint = {1405.0312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.0312},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2025-04-15},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{manjulaNovelHashBased2015,
  title = {A Novel Hash Based Least Significant Bit (2-3-3) Image Steganography in Spatial Domain},
  author = {Manjula, G. R. and Danti, Ajit},
  date = {2015-02-28},
  journaltitle = {IJSPTM},
  volume = {4},
  number = {1},
  eprint = {1503.03674},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {11--20},
  issn = {23194103, 22775498},
  doi = {10.5121/ijsptm.2015.4102},
  url = {http://arxiv.org/abs/1503.03674},
  urldate = {2024-12-03},
  abstract = {This paper presents a novel 2-3-3 LSB insertion method. The image steganography takes the advantage of human eye limitation. It uses color image as cover media for embedding secret message.The important quality of a steganographic system is to be less distortive while increasing the size of the secret message. In this paper a method is proposed to embed a color secret image into a color cover image. A 2-3-3 LSB insertion method has been used for image steganography. Experimental results show an improvement in the Mean squared error (MSE) and Peak Signal to Noise Ratio (PSNR) values of the proposed technique over the base technique of hash based 3-3-2 LSB insertion.},
  keywords = {Computer Science - Multimedia}
}

@online{Midjourney,
  title = {Midjourney},
  url = {https://www.midjourney.com/website},
  urldate = {2025-07-21},
  abstract = {An independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.},
  organization = {Midjourney}
}

@article{mohanarathinamDigitalWatermarkingTechniques2020,
  title = {Digital Watermarking Techniques for Image Security: A Review},
  shorttitle = {Digital Watermarking Techniques for Image Security},
  author = {Mohanarathinam, A. and Kamalraj, S. and Prasanna Venkatesan, G. K. D. and Ravi, Renjith V. and Manikandababu, C. S.},
  date = {2020-08-01},
  journaltitle = {J Ambient Intell Human Comput},
  volume = {11},
  number = {8},
  pages = {3221--3229},
  issn = {1868-5145},
  doi = {10.1007/s12652-019-01500-1},
  url = {https://doi.org/10.1007/s12652-019-01500-1},
  urldate = {2024-12-10},
  abstract = {Multimedia technology usages is increasing day by day and to provide authorized data and protecting the secret information from unauthorized use is highly difficult and involves a complex process. By using the watermarking technique, only authorized user can use the data. Digital watermarking is a widely used technology for the protection of digital data. Digital watermarking deals with the embedding of secret data into actual information. Digital watermarking techniques are classified into three major categories, and they were based on domain, type of document (text, image, music or video) and human perception. Performance of the watermarked images is analysed using Peak signal to noise ratio, mean square error and bit error rate. Watermarking of images has been researched profoundly for its specialized and modern achievability in all media applications such as copyrights protection, medical reports (MRI scan and X-ray), annotation and privacy control. This paper reviews the watermarking technique and its merits and demerits.},
  langid = {english},
  keywords = {Artificial Intelligence,BER,DCT,DWT,Fragile watermarking,Hybrid watermarking,LSB,MSE,PSNR,Robust watermarking,SVD,Watermarking}
}

@online{panthiWatermarkingDiffusionModel2025,
  title = {Watermarking in {{Diffusion Model}}: {{Gaussian Shading}} with {{Exact Diffusion Inversion}} via {{Coupled Transformations}} ({{EDICT}})},
  shorttitle = {Watermarking in {{Diffusion Model}}},
  author = {Panthi, Krishna},
  date = {2025-01-15},
  eprint = {2501.08604},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.08604},
  url = {http://arxiv.org/abs/2501.08604},
  urldate = {2025-07-08},
  abstract = {This paper introduces a novel approach to enhance the performance of Gaussian Shading, a prevalent watermarking technique, by integrating the Exact Diffusion Inversion via Coupled Transformations (EDICT) framework. While Gaussian Shading traditionally embeds watermarks in a noise latent space, followed by iterative denoising for image generation and noise addition for watermark recovery, its inversion process is not exact, leading to potential watermark distortion. We propose to leverage EDICT's ability to derive exact inverse mappings to refine this process. Our method involves duplicating the watermark-infused noisy latent and employing a reciprocal, alternating denoising and noising scheme between the two latents, facilitated by EDICT. This allows for a more precise reconstruction of both the image and the embedded watermark. Empirical evaluation on standard datasets demonstrates that our integrated approach yields a slight, yet statistically significant improvement in watermark recovery fidelity. These results highlight the potential of EDICT to enhance existing diffusion-based watermarking techniques by providing a more accurate and robust inversion mechanism. To the best of our knowledge, this is the first work to explore the synergy between EDICT and Gaussian Shading for digital watermarking, opening new avenues for research in robust and high-fidelity watermark embedding and extraction.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@online{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2102.12092},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.12092},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2024-12-09},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{reedPolynomialCodesCertain1960,
  title = {Polynomial {{Codes Over Certain Finite Fields}}},
  author = {Reed, I. S. and Solomon, G.},
  date = {1960-06},
  journaltitle = {Journal of the Society for Industrial and Applied Mathematics},
  volume = {8},
  number = {2},
  pages = {300--304},
  publisher = {Society for Industrial \& Applied Mathematics (SIAM)},
  issn = {0368-4245, 2168-3484},
  doi = {10.1137/0108018},
  url = {http://epubs.siam.org/doi/10.1137/0108018},
  urldate = {2025-07-20},
  langid = {english}
}

@misc{RegulationEU20242024,
  title = {Regulation ({{EU}}) 2024/1689 of the {{European Parliament}} and of the {{Council}} of 13 {{June}} 2024 Laying down Harmonised Rules on Artificial Intelligence and Amending {{Regulations}} ({{EC}}) {{No}} 300/2008, ({{EU}}) {{No}} 167/2013, ({{EU}}) {{No}} 168/2013, ({{EU}}) 2018/858, ({{EU}}) 2018/1139 and ({{EU}}) 2019/2144 and {{Directives}} 2014/90/{{EU}}, ({{EU}}) 2016/797 and ({{EU}}) 2020/1828 ({{Artificial Intelligence Act}}) ({{Text}} with {{EEA}} Relevance)},
  date = {2024-06-13},
  url = {http://data.europa.eu/eli/reg/2024/1689/oj/eng},
  urldate = {2024-12-05},
  langid = {english}
}

@online{rezaeiLaWaUsingLatent2024,
  title = {{{LaWa}}: {{Using Latent Space}} for {{In-Generation Image Watermarking}}},
  shorttitle = {{{LaWa}}},
  author = {Rezaei, Ahmad and Akbari, Mohammad and Alvar, Saeed Ranjbar and Fatemi, Arezou and Zhang, Yong},
  date = {2024-08-22},
  eprint = {2408.05868},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.05868},
  url = {http://arxiv.org/abs/2408.05868},
  urldate = {2024-12-03},
  abstract = {With generative models producing high quality images that are indistinguishable from real ones, there is growing concern regarding the malicious usage of AI-generated images. Imperceptible image watermarking is one viable solution towards such concerns. Prior watermarking methods map the image to a latent space for adding the watermark. Moreover, Latent Diffusion Models (LDM) generate the image in the latent space of a pre-trained autoencoder. We argue that this latent space can be used to integrate watermarking into the generation process. To this end, we present LaWa, an in-generation image watermarking method designed for LDMs. By using coarse-to-fine watermark embedding modules, LaWa modifies the latent space of pre-trained autoencoders and achieves high robustness against a wide range of image transformations while preserving perceptual quality of the image. We show that LaWa can also be used as a general image watermarking method. Through extensive experiments, we demonstrate that LaWa outperforms previous works in perceptual quality, robustness against attacks, and computational complexity, while having very low false positive rate. Code is available here.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@online{rezendeStochasticBackpropagationApproximate2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  date = {2014-05-30},
  eprint = {1401.4082},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1401.4082},
  url = {http://arxiv.org/abs/1401.4082},
  urldate = {2024-11-16},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@online{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2022-04-13},
  eprint = {2112.10752},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2112.10752},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2024-11-28},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{saqibSpatialFrequencyDomain2017,
  title = {Spatial and {{Frequency Domain Digital Image Watermarking Techniques}} for {{Copyright Protection}}},
  author = {Saqib, Manasha and Naaz, Sameena},
  date = {2017-06-01},
  volume = {9},
  pages = {691--699},
  abstract = {Digital image watermarking is considered to be one of the extremely valuable technology for providing security to the digital media on web. The act of concealing a message related to a digital signals in various forms such as an image, video, song inside the signal itself is known as Digital image watermarking. Watermarking is significant application in the image processing. Watermarking is the process of embedding predefined designs into multimedia information in a way that the degradation of image quality is reduced and remain at an imperceptible level. It is fundamentally required to shield the data from the unauthorized access. The important issues to be considered while performing digital image watermarking are imperceptibility, capacity robustness, and security. In this paper, an overview of an image watermarking, types of watermarking and necessities of digital watermarking is presented. It presents various techniques of digital image watermarking depending upon spatial \& frequency domain.}
}

@online{ScoreDetectBlogIntellectual2024,
  title = {{{ScoreDetect Blog}} | {{Intellectual Property}} \& {{Copyright Protection}}},
  date = {2024-09-07},
  url = {https://www.scoredetect.com/blog/posts/can-you-copyright-ai-art-legal-insights},
  urldate = {2024-11-27},
  abstract = {View the latest IP and copyright protection news and updates at ScoreDetect blog.},
  langid = {english},
  organization = {ScoreDetect Blog | Intellectual Property \& Copyright Protection}
}

@inproceedings{shanGlazeProtectingArtists2023,
  title = {Glaze: {{Protecting Artists}} from {{Style Mimicry}} by \{\vphantom\}{{Text-to-Image}}\vphantom\{\} {{Models}}},
  shorttitle = {Glaze},
  author = {Shan, Shawn and Cryan, Jenna and Wenger, Emily and Zheng, Haitao and Hanocka, Rana and Zhao, Ben Y.},
  date = {2023},
  pages = {2187--2204},
  url = {https://www.usenix.org/conference/usenixsecurity23/presentation/shan},
  urldate = {2024-11-15},
  eventtitle = {32nd {{USENIX Security Symposium}} ({{USENIX Security}} 23)},
  isbn = {978-1-939133-37-3},
  langid = {english}
}

@online{shanNightshadePromptSpecificPoisoning2024,
  title = {Nightshade: {{Prompt-Specific Poisoning Attacks}} on {{Text-to-Image Generative Models}}},
  shorttitle = {Nightshade},
  author = {Shan, Shawn and Ding, Wenxin and Passananti, Josephine and Wu, Stanley and Zheng, Haitao and Zhao, Ben Y.},
  date = {2024-04-29},
  eprint = {2310.13828},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2310.13828},
  url = {http://arxiv.org/abs/2310.13828},
  urldate = {2024-11-15},
  abstract = {Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts. We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in {$<$}100 poison samples. Nightshade poison effects "bleed through" to related concepts, and multiple attacks can composed together in a single prompt. Surprisingly, we show that a moderate number of Nightshade attacks can destabilize general features in a text-to-image generative model, effectively disabling its ability to generate meaningful images. Finally, we propose the use of Nightshade and similar tools as a last defense for content creators against web scrapers that ignore opt-out/do-not-crawl directives, and discuss possible implications for model trainers and content creators.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security}
}

@article{shumailovAIModelsCollapse2024,
  title = {{{AI}} Models Collapse When Trained on Recursively Generated Data},
  author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  date = {2024-07},
  journaltitle = {Nature},
  volume = {631},
  number = {8022},
  pages = {755--759},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07566-y},
  url = {https://www.nature.com/articles/s41586-024-07566-y},
  urldate = {2024-11-29},
  abstract = {Stable diffusion revolutionized image creation from descriptive text. GPT-2 (ref.\,1), GPT-3(.5) (ref.\,2) and GPT-4 (ref.\,3) demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here we consider what may happen to GPT-\{n\} once LLMs contribute much of the text found online. We find that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. We refer to this effect as ‘model collapse’ and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). We build theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. We demonstrate that it must be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.},
  langid = {english},
  keywords = {Computational science,Computer science}
}

@online{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015-11-18},
  eprint = {1503.03585},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1503.03585},
  url = {http://arxiv.org/abs/1503.03585},
  urldate = {2024-11-16},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning}
}

@online{songDenoisingDiffusionImplicit2022,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  date = {2022-10-05},
  eprint = {2010.02502},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.02502},
  url = {http://arxiv.org/abs/2010.02502},
  urldate = {2025-04-15},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 \textbackslash times\$ to \$50 \textbackslash times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@online{SurveyDigitalImage,
  title = {A Survey of Digital Image Watermarking Techniques | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/1560462},
  urldate = {2024-12-04}
}

@online{szegedyIntriguingPropertiesNeural2014,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  date = {2014-02-19},
  eprint = {1312.6199},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1312.6199},
  url = {http://arxiv.org/abs/1312.6199},
  urldate = {2024-11-15},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@online{ThousandsArtistsCondemn2024,
  title = {Thousands of {{Artists Condemn Unlicensed Use}} of {{Their Work}} to {{Train A}}.{{I}}.},
  date = {2024-10-22T18:16:10+00:00},
  url = {https://news.artnet.com/art-world/artists-ai-statement-2557164},
  urldate = {2025-07-18},
  abstract = {More than 11,000 artists signed an open letter warning A.I.companies that the unlicensed use of their work is "unjust."},
  langid = {american},
  organization = {Artnet News}
}

@online{wadheraComprehensiveReviewDigital2022,
  title = {A {{Comprehensive Review}} on {{Digital Image Watermarking}}},
  author = {Wadhera, Shweta and Kamra, Deepa and Rajpal, Ankit and Jain, Aruna and Jain, Vishal},
  date = {2022-07-07},
  eprint = {2207.06909},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.06909},
  url = {http://arxiv.org/abs/2207.06909},
  urldate = {2024-12-10},
  abstract = {The advent of the Internet led to the easy availability of digital data like images, audio, and video. Easy access to multimedia gives rise to the issues such as content authentication, security, copyright protection, and ownership identification. Here, we discuss the concept of digital image watermarking with a focus on the technique used in image watermark embedding and extraction of the watermark. The detailed classification along with the basic characteristics, namely visual imperceptibility, robustness, capacity, security of digital watermarking is also presented in this work. Further, we have also discussed the recent application areas of digital watermarking such as healthcare, remote education, electronic voting systems, and the military. The robustness is evaluated by examining the effect of image processing attacks on the signed content and the watermark recoverability. The authors believe that the comprehensive survey presented in this paper will help the new researchers to gather knowledge in this domain. Further, the comparative analysis can enkindle ideas to improve upon the already mentioned techniques.},
  pubstate = {prepublished},
  keywords = {Computer Science - Multimedia,Electrical Engineering and Systems Science - Signal Processing}
}

@article{wallaceJPEGStillPicture1991,
  title = {The {{JPEG}} Still Picture Compression Standard},
  author = {Wallace, Gregory K.},
  date = {1991-04-01},
  journaltitle = {Commun. ACM},
  volume = {34},
  number = {4},
  pages = {30--44},
  issn = {0001-0782},
  doi = {10.1145/103085.103089},
  url = {https://dl.acm.org/doi/10.1145/103085.103089},
  urldate = {2024-12-04}
}

@article{wenTreeRingsWatermarksInvisible2023,
  title = {Tree-{{Rings Watermarks}}: {{Invisible Fingerprints}} for {{Diffusion Images}}},
  shorttitle = {Tree-{{Rings Watermarks}}},
  author = {Wen, Yuxin and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {58047--58063},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/b54d1757c190ba20dbc4f9e4a2f54149-Abstract-Conference.html},
  urldate = {2024-12-10},
  langid = {english}
}

@article{xiaWaveletTransformBased1998,
  title = {Wavelet Transform Based Watermark for Digital Images},
  author = {Xia, Xiang-Gen and Boncelet, Charles G. and Arce, Gonzalo R.},
  date = {1998-12-07},
  journaltitle = {Opt. Express, OE},
  volume = {3},
  number = {12},
  pages = {497--511},
  publisher = {Optica Publishing Group},
  issn = {1094-4087},
  doi = {10.1364/OE.3.000497},
  url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-3-12-497},
  urldate = {2024-11-16},
  abstract = {In this paper, we introduce a new multiresolution watermarking method for digital images. The method is based on the discrete wavelet transform (DWT). Pseudo-random codes are added to the large coefficients at the high and middle frequency bands of the DWT of an image. It is shown that this method is more robust to proposed methods to some common image distortions, such as the wavelet transform based image compression, image rescaling/stretching and image halftoning. Moreover, the method is hierarchical.},
  langid = {english}
}

@online{xuInvisMarkInvisibleRobust2024,
  title = {{{InvisMark}}: {{Invisible}} and {{Robust Watermarking}} for {{AI-generated Image Provenance}}},
  shorttitle = {{{InvisMark}}},
  author = {Xu, Rui and Hu, Mengya and Lei, Deren and Li, Yaxi and Lowe, David and Gorevski, Alex and Wang, Mingyu and Ching, Emily and Deng, Alex},
  date = {2024-11-19},
  eprint = {2411.07795},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.07795},
  url = {http://arxiv.org/abs/2411.07795},
  urldate = {2024-12-09},
  abstract = {The proliferation of AI-generated images has intensified the need for robust content authentication methods. We present InvisMark, a novel watermarking technique designed for high-resolution AI-generated images. Our approach leverages advanced neural network architectures and training strategies to embed imperceptible yet highly robust watermarks. InvisMark achieves state-of-the-art performance in imperceptibility (PSNR\$\textbackslash sim\$51, SSIM \$\textbackslash sim\$ 0.998) while maintaining over 97\textbackslash\% bit accuracy across various image manipulations. Notably, we demonstrate the successful encoding of 256-bit watermarks, significantly expanding payload capacity while preserving image quality. This enables the embedding of UUIDs with error correction codes, achieving near-perfect decoding success rates even under challenging image distortions. We also address potential vulnerabilities against advanced attacks and propose mitigation strategies. By combining high imperceptibility, extended payload capacity, and resilience to manipulations, InvisMark provides a robust foundation for ensuring media provenance in an era of increasingly sophisticated AI-generated content. Source code of this paper is available at: https://github.com/microsoft/InvisMark.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security}
}

@online{yangGaussianShadingProvable2024,
  title = {Gaussian {{Shading}}: {{Provable Performance-Lossless Image Watermarking}} for {{Diffusion Models}}},
  shorttitle = {Gaussian {{Shading}}},
  author = {Yang, Zijin and Zeng, Kai and Chen, Kejiang and Fang, Han and Zhang, Weiming and Yu, Nenghai},
  date = {2024-05-06},
  eprint = {2404.04956},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.04956},
  url = {http://arxiv.org/abs/2404.04956},
  urldate = {2025-04-14},
  abstract = {Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security}
}

@online{zhangAttackResilientImageWatermarking2024,
  title = {Attack-{{Resilient Image Watermarking Using Stable Diffusion}}},
  author = {Zhang, Lijun and Liu, Xiao and Martin, Antoni Viros and Bearfield, Cindy Xiong and Brun, Yuriy and Guan, Hui},
  date = {2024-10-28},
  eprint = {2401.04247},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.04247},
  url = {http://arxiv.org/abs/2401.04247},
  urldate = {2024-12-03},
  abstract = {Watermarking images is critical for tracking image provenance and proving ownership. With the advent of generative models, such as stable diffusion, that can create fake but realistic images, watermarking has become particularly important to make human-created images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate above 98\% and a false positive rate below 6.4\%, outperforming state-of-the-art watermarking methods. We hypothesize that the reciprocating denoising process in diffusion models may inherently enhance the robustness of the watermark when faced with strong attacks and validate the hypothesis. Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion--based attack methods. ZoDiac is open-sourced and available at https://github.com/zhanglijun95/ZoDiac.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@article{zhangPerceptualImageWatermarking2023,
  title = {Towards Perceptual Image Watermarking with Robust Texture Measurement},
  author = {Zhang, Yunming and Gong, Yuxin and Wang, Jun and Sun, Jiande and Wan, Wenbo},
  date = {2023-06-01},
  journaltitle = {Expert Systems with Applications},
  volume = {219},
  pages = {119649},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.119649},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417423001501},
  urldate = {2024-12-03},
  abstract = {Improving the perceptual quality of images is challenging for the robust watermarking method. To address this problem, most existing algorithms proposed to guide the watermark embedding based on just noticeable difference (JND)~model, which mainly consists of corresponding contrast masking factor. However, existing metrics are mostly based on the intra-block texture energy, which cannot exactly describe the texture information and meet the needs of watermarking robustness. In this paper, we present a new texture metric, which incorporates the visual saliency based inter-block texture regularity with the robust discrete cosine transform (DCT) coefficients. The JND model is constructed as a modulation factor by using the proposed robust texture metric, which embeds the intra-block energy and inter-block regularity measurement to capture local and global features. Based on the proposed model, we introduce an adaptive quantization step for the watermarking framework. Both quantitative and qualitative results show that our scheme achieves better performance in comparison with several state-of-the-art schemes.},
  keywords = {Contrast masking,Image watermarking,JND model,Robustness,Texture measurement}
}

@software{zhangZhanglijun95ZoDiac2024,
  title = {Zhanglijun95/{{ZoDiac}}},
  author = {Zhang, Ivy},
  date = {2024-11-26T15:04:19Z},
  origdate = {2024-02-19T18:54:58Z},
  url = {https://github.com/zhanglijun95/ZoDiac},
  urldate = {2024-12-03},
  abstract = {Attack-Resilient Image Watermarking Using Stable Diffusion (NeurIPS2024)}
}

@online{zhaoRecipeWatermarkingDiffusion2023,
  title = {A {{Recipe}} for {{Watermarking Diffusion Models}}},
  author = {Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Cheung, Ngai-Man and Lin, Min},
  date = {2023-10-15},
  eprint = {2303.10137},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2303.10137},
  url = {http://arxiv.org/abs/2303.10137},
  urldate = {2024-11-28},
  abstract = {Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at https://github.com/yunqing-me/WatermarkDM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{zhaoRecipeWatermarkingDiffusion2023a,
  title = {A {{Recipe}} for {{Watermarking Diffusion Models}}},
  author = {Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Cheung, Ngai-Man and Lin, Min},
  date = {2023-10-15},
  eprint = {2303.10137},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.10137},
  url = {http://arxiv.org/abs/2303.10137},
  urldate = {2024-12-10},
  abstract = {Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at https://github.com/yunqing-me/WatermarkDM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@online{zhaoSoKWatermarkingAIGenerated2024,
  title = {{{SoK}}: {{Watermarking}} for {{AI-Generated Content}}},
  shorttitle = {{{SoK}}},
  author = {Zhao, Xuandong and Gunn, Sam and Christ, Miranda and Fairoze, Jaiden and Fabrega, Andres and Carlini, Nicholas and Garg, Sanjam and Hong, Sanghyun and Nasr, Milad and Tramer, Florian and Jha, Somesh and Li, Lei and Wang, Yu-Xiang and Song, Dawn},
  date = {2024-11-27},
  eprint = {2411.18479},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2411.18479},
  url = {http://arxiv.org/abs/2411.18479},
  urldate = {2024-11-29},
  abstract = {As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@article{zhengRSTinvariantDigitalImage2003,
  title = {{{RST-invariant}} Digital Image Watermarking Based on Log-Polar Mapping and Phase Correlation},
  author = {Zheng, D. and Zhao, J. and El Saddik, A.},
  date = {2003-08},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {13},
  number = {8},
  pages = {753--765},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2003.815959},
  url = {https://ieeexplore.ieee.org/document/1227605},
  urldate = {2024-12-07},
  abstract = {Based on log-polar mapping (LPM) and phase correlation, the paper presents a novel digital image watermarking scheme that is invariant to rotation, scaling, and translation (RST). We embed a watermark in the LPMs of the Fourier magnitude spectrum of an original image, and use the phase correlation between the LPM of the original image and the LPM of the watermarked image to calculate the displacement of watermark positions in the LPM domain. The scheme preserves the image quality by avoiding computing the inverse log-polar mapping (ILPM), and produces smaller correlation coefficients for unwatermarked images by using phase correlation to avoid exhaustive search. The evaluations demonstrate that the scheme is invariant to rotation and translation, invariant to scaling when the scale is in a reasonable range, and very robust to JPEG compression.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  keywords = {Digital images,Discrete Fourier transforms,Fourier transforms,Image coding,Image quality,Noise robustness,Transform coding,Video compression,Watermarking}
}

@online{zhuHiDDeNHidingData2018,
  title = {{{HiDDeN}}: {{Hiding Data With Deep Networks}}},
  shorttitle = {{{HiDDeN}}},
  author = {Zhu, Jiren and Kaplan, Russell and Johnson, Justin and Fei-Fei, Li},
  date = {2018-07-26},
  eprint = {1807.09937},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1807.09937},
  url = {http://arxiv.org/abs/1807.09937},
  urldate = {2024-11-14},
  abstract = {Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}
