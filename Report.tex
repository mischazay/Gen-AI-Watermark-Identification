\documentclass{UoYCSproject}

\setlength{\marginparwidth}{2cm}
\usepackage{todonotes} % to be removed, used for adding markers for references needed
\usepackage{marginfix}

\usepackage[nohyperlinks]{acronym} % addition for acronyms page
\usepackage{hyperref}

\addbibresource{bibliography.bib}
\author{Mischa}
\title{Identifying images generated by AI using watermarks}
\date{Version 3.0, 2020-November}
\supervisor{Dimitar Kazakov}
\BSc

\dedication{To all students everywhere}

\acknowledgements{
  I would like to thank my goldfish for all the help it gave me
  writing this document.
 
  As usual, my boss was an inspiring source of sagacious advice.
}

% More definitions & declarations in example.ldf

\begin{document}
\pagenumbering{roman}
\maketitle
\listoffigures
\listoftables
%\renewcommand*{\lstlistlistingname}{List of Listings}
%\lstlistoflistings

% List of Acronyms used
\chapter*{List of Acronyms}
\addcontentsline{toc}{chapter}{List of Acronyms}
\begin{acronym}[XXXXX]  % Use longest acronym width to suppress error
  \acro{AI}{Artificial Intelligence}
  \acro{GenAI}{Generative AI}
  \acro{GAIM}{Generative AI Image Model}
  \acro{GAII}{Generative AI Image}
  \acro{VAE}{Variational Autoencoder}
  \acro{GAN}{Generative Adversarial Network}
  \acro{DM}{Diffusion Model}
  \acro{LDM}{Latent Diffusion Model}
  \acro{RGB}{Red, Green, Blue}
  \acro{LSB}{Least Significant Bit}
  \acro{DCT}{Discrete Cosine Transform}
  \acro{DFT}{Discrete Fourier Transform}
  \acro{DWT}{Discrete Wavelet Transform}
  \acro{LPM}{Log-Polar Mapping}
  \acro{MSE}{Mean Squared Error}
  \acro{PSNR}{Peak Signal-to-Noise-Ratio}
  \acro{SSIM}{Structural Similarity Index Measure}
  \acro{NCC}{Normalised Cross-Correlation}
\end{acronym}

\begin{summary}
test
\end{summary}


\begin{ethics}

\end{ethics}


\chapter{Introduction}
\label{cha:Introduction}
\acp{GAIM} such as Midjourney, DALL-E \cite{rameshZeroShotTexttoImageGeneration2021}, and Stable Diffusion \cite{rombachHighResolutionImageSynthesis2022} are capable of generating highly realistic images. While these technologies have enabled new possibilities in fields such as content creation, design prototyping, and automation, they also raise significant ethical concerns. A key issue is the potential misuse of these models for generating malicious or misleading content to deceive the public \cite{ferraraGenAIHumanityNefarious2024}. Current ruling by the US Copyright Office makes \acp{GAII} illegible for copyright protections \cite{CopyrightRegistrationGuidance2023}.

In response to these concerns, watermarking has emerged as a promising solution for establishing authenticity, attribution, and traceability in \acp{GAII}. The US White House recommended watermarking as part of its 2023 Executive order on \ac{AI} governance \cite{houseExecutiveOrderSafe2023}. Similarly, the EU's AI Act 2024 mandates that providers of AI systems generating synthetic content embed invisible watermarks \cite{RegulationEU20242024}. Leading tech companies have adopted watermarking methods: Microsoft watermarks images created in Bing \cite{BingPreviewRelease2023}, Google watermarks its \ac{GenAI} images using their SynthID \cite{IdentifyingAIgeneratedImages2024}, and Stability AI watermarks outputs from Stable Diffusion \cite{CompVisStablediffusion2024}.

Digital watermarking involves embedding hidden information within digital content, a practice that dates back to physical media such as watermarked paper and photographs. This same need translated to digital media, particularly to protect intellectual property and prevent unauthorised use. Foundational work by Cox et al. \cite{coxDigitalWatermarking2001} established core principles for digital watermarking, emphasising resistance to common manipulations such as compression, resizing.

The widespread adoption of \acp{GAIM} intensifies concerns about content authenticity, copyright infringement, and attribution. As generative models such as \acp{GAN}, \acp{VAE}, and \acp{DM} advance, the boundaries between human-created and \ac{AI}-generated content are increasingly blurred. Artists have expressed concerns about unauthorised use of their work in \ac{AI} training datasets \todo{citation needed}, resulting in \ac{AI}-generated images mimicking their styles without appropriate attribution or compensation \todo{citation needed}. This has lead to calls for greater transparency and accountability in the creation and use of \ac{AI}-generated content.

Adversarial image poisoning methods such as Glaze \cite{shanGlazeProtectingArtists2023} and Nightshade \cite{shanNightshadePromptSpecificPoisoning2024} further showcase the desire for more transparency regarding image training data. These methods both use adversarial perturbations (tiny changes to the input) to change a given models perception of the original image \cite{kurakinAdversarialExamplesPhysical2017}.

This project aims to explore the integration of digital watermarking techniques with \acp{GAIM} to address issues of authenticity, attribution, traceability and copyright protection. Digital watermarking involves embedding information into digital media, enabling retrieval through specific algorithms while preserving the visual quality of the image. Secondly, the project aims to evaluate the effectiveness of said watermarking implementation through rigorous testing and analysis. Including assessing the robustness of the watermarks against various attacks, such as compression, noise addition, and cropping as well as its capacity for embedding meaningful amounts of data without compromising the visual quality of generated images.


\chapter{Literature Review}
\label{cha:Literature Review}
\section{Background}

\subsection{Generative Models in AI Image Generation}
Generative images have revolutionised the \ac{AI} field by enabling the creation of new data that closely resembles the training data. The three primary generative models used in \ac{AI} image generation being: \acp{GAN}, \acp{VAE}, and \acp{DM}.

% \subsubsection{Variational Autoencoders (VAEs)}
\subsubsection{Variational Autoencoders (VAEs)}
\acp{VAE} were first defined in 2013 by Kingma et al. \cite{kingmaAutoEncodingVariationalBayes2022} and Rezende et al. \cite{rezendeStochasticBackpropagationApproximate2014}. \acp{VAE} are probabilistic generative models that learn a latent space representation of input data. They consist of an encoder and a decoder, which work together to reconstruct input data from a compressed latent space. Watermarking in \acp{VAE} could involve perturbing latent space to insert information \cite{guoFreqMarkInvisibleImage2024}.

\subsubsection{Generative Adversarial Networks}
\acp{GAN}, proposed by Goodfellow et al. \cite{goodfellowGenerativeAdversarialNetworks2014} in 2014. \acp{GAN} consist of two neural networks: A generator, and a discriminator. The generator takes an input of random noise and generates an image by reassembling the real data distribution; The discriminator seeks to differentiate between real and generated images. The competing nature of the model helps in improving the quality of images generated over time. However, images generated are sensitive to perturbations \cite{alfarraRobustnessQualityMeasures2022}. Therefore, adding a watermark could degrade the quality of the image.

\subsubsection{Diffusion Models}
\acp{DM} were first introduced in 2015 by Sohl-Dickstein et al. \cite{sohl-dicksteinDeepUnsupervisedLearning2015} and popularised in 2020 by Ho et al. \cite{hoDenoisingDiffusionProbabilistic2020}. Unlike \acp{GAN} and \acp{VAE}, \acp{DM} generate images by iteratively denoising a random noise pattern, producing high quality images with fine details. The challenge in watermarking \acp{DM} lies in ensuring the watermark does not interfere with the denoising process in order to preserve image quality. \acp{LDM} such as Stable Diffusion, extend the concept of diffusion by performing the denoising process in a latent space rather than directly in pixel space, allowing for more efficient training and inference, as well as improved image quality \cite{rombachHighResolutionImageSynthesis2022}. In \acp{LDM}, a \ac{VAE} is used to compress the image to into a lower-dimensional latent space, which is then denoised iteratively to generate the final image.

\subsection{Need for watermarking in AI era}
\subsection{Preventing Training Data Contamination}
Lots of \acp{GAIM} rely on publicly available data scraped on the internet for training. When models are inadvertently trained on their own or other image models outputs, "model collapse" can happen, where repetitive cycles of training degrade the quality of generated content \cite{bohacekNepotisticallyTrainedGenerativeAI2023}. Watermarking \acp{GAII} can mitigate this risk by marking synthetic outputs, enabling the dataset curation process to filter out these images for training.


\subsection{Attribution and Traceability}
Watermarks can act as unique identifiers for \acp{GAII}.

Depending on implementation, watermarks could act as unique identifiers to identify the: model/platform, training data, user and time of generation for a given image. Providing greater accountability, especially relating to concerns of images being trained on licensed data.

The Coalition for Content Provenance and Authenticity (C2PA) is a cross industry initiative formed by major tech companies, including Microsoft, Arm, Intel and Adobe, aiming to establish a standard framework for certifying the origin and history of digital content. \cite{ContentCredentialsC2PA} \todo{finish}

Watermarking \acp{GAII} has several critical applications, particularly in the realm of copyright protection, ownership, authenticity, and traceability.

An effective watermarking implementation would provide a means for \todo{finish}

A particularly interesting aspect of \ac{GAII} watermarking is attribution, the ability to trace back the creator of a given image \cite{jiangWatermarkbasedDetectionAttribution2024}


\section{Watermarking Techniques}
\subsection{Spatial Domain Watermarking Techniques}
Spatial domain watermarking involves embedding watermarks directly into the pixel values. These methods are straightforward, easy to implement, and computationally efficient. However, are typically less resistant to attacks such as compression and transformations.

\subsubsection{Least Significant Bit (LSB) Modification}
A common technique to embed a watermark information into randomly chosen pixels' \ac{LSB}. The \ac{LSB} is changed as to not affect the image quality as it contains less important information. However, it is trivial for an attacker to change all \ac{LSB} bits to 1 to modify the watermark. To address the problems with \ac{LSB} watermarking, improvements have been made. One such improvement embeds data not only to the \ac{LSB} but also higher planes. Moreover, a 2-3-3 embedding technique \cite{manjulaNovelHashBased2015} distributes the watermark across the \ac{RGB} channels of a pixel. This approach results in minimal perceptual distortion while achieving better embedding capacity and robustness.

\subsubsection{Patch-based or Block-Based techniques}
Proposed by Bender et al. \cite{benderTechniquesDataHiding1996} This method involves randomly picking $n$ pairs of image points $A,B$ where the image data in $A$ is darkened, while is brightened in $B$. This method offers decent robustness in exchange for capacity. \cite{saqibSpatialFrequencyDomain2017}

\subsection{Frequency (or Transform) Domain Watermarking Techniques}
These techniques embed watermark information within the frequency domain of an image after a transformation. The transformation spreads the watermark information throughout the image in ways that are less perceptible to the human eye and harder to remove with common attacks.

\subsubsection{Discrete Cosine Transform}
\ac{DCT} watermarking embeds watermark information into an images frequency coefficients after transforming it from the spatial to the frequency domain. This leverages energy compaction, where the majority of an image's visual information is represented by lower-frequency coefficients, while higher-frequency coefficients capture finer image details. A common approach is block-based \ac{DCT}, where the image is divided into smaller non-overlapping blocks, \ac{DCT} is then applied to each block. Mid-frequency coefficients are typically chosen, balancing imperceptibility and robustness. Modifying low-frequency coefficients can lead to more noticeable distortions, while high-frequency coefficients are more susceptible to compression and noise attacks. Block-based \ac{DCT} is particularly suitable for JPEG compression, a prevalent image compression technique which is also block-based \cite{wallaceJPEGStillPicture1991}. By embedding watermarks in \ac{DCT} coefficients compatible with JPEG's compression algorithm, the watermark can survive compression without significant degradation \cite{borsImageWatermarkingUsing1996}. Alternatively, global \ac{DCT} applies the transformation to the entire image rather than individual blocks. This offers greater robustness against attacks, but is more computationally intensive and less compatible with block-based compression techniques such as JPEG.

The robustness of \ac{DCT}-based watermarking comes from the ability to embed data in perceptually significant regions of an image, therefore being less likely to be removed by common image processing operations. However, \ac{DCT} based watermarking methods struggle with maintaining robustness against geometric attacks such as scaling and rotation due to inherently not accounting for spatial transformations \cite{fazliRobustImageWatermarking2016}. From this hybrid techniques combining \ac{DCT} with other transformations have arisen \cite{abdulrahmanNovelHybridDCT2019}.

\subsubsection{Discrete Fourier Transform}
Similar to \ac{DCT} watermarking, \ac{DFT} embeds watermark information into an images frequency domain by transforming it from the spatial domain to the frequency domain, but using the \ac{DFT} which decomposes an image into sinusoidal components of varying frequencies, represented as complex-valued coefficients corresponding to magnitude and phase. These coefficients describe the global frequency characteristics of the image, making \ac{DFT}-based watermarking inherently robust against various image processing operations and certain geometric transformations.

\ac{LPM} transforms the image into log-polar coordinates before applying \ac{DFT}. This mapping converts scaling and rotation into linear translations in the frequency domain, enabling efficient watermark extraction after significant geometric transformations \cite{zhengRSTinvariantDigitalImage2003}.

\subsubsection{Discrete Wavelet Transform}
A \ac{DWT} is any wavelet transform that decomposes a signal into wavelets, offering local analysis in both the time and frequency domains. Unlike \ac{DFT}, which analyses global frequency count, and \ac{DCT} which can operate globally or block-based, \ac{DWT} inherently supports multi-resolution analysis by examining signals at different scales. This dual localisation makes \ac{DWT} particularly effective for image watermarking, as it can capture coarse and fine image details simultaneously.

\subsubsection{Applications to AI-Generated Images}
Traditional frequency domain methods (DCT, DWT) represent conventional image watermarking approaches that operate in the frequency domain. While well-established and widely used for natural images, these techniques are generic post-processing methods that don't specifically leverage the properties of the AI generation processes. Their application remains relatively unexplored in recent LDM-specific literature compared to direct latent modification approaches that are purposefully designed for AI-generated content.

\subsection{State-of-the-Art Watermarking Techniques for AI-Generated Images}
\subsubsection{HiDDeN}
One promising advancement in watermarking is the HiDDeN framework \cite{zhuHiDDeNHidingData2018}. HiDDeN leverages the sensitivity of deep neural networks to small perturbations in input images to encode information, making it a robust solution for watermarking.

The HiDDeN framework comprises three main components: an encoder, a decoder, and an adversary network. The encoder receives an image and a message string, outputting an encoded image that incorporates the watermark. The decoder attempts to reconstruct the original message from the encoded image, while the adversary network predicts whether a given image contains an encoded watermark, providing adversarial loss to enhance the quality of the encoded images.

The adversarial training enhances the watermark's resilience against numerous attacks. The deep leaning approach allows for a more flexible watermark embedding,

\subsubsection{Post-Generation Methods}
While Methods like HiDDeN \cite{zhuHiDDeNHidingData2018} and FreqMark \cite{guoFreqMarkInvisibleImage2024} demonstrate impressive results, they are primarily designed as post-generation solutions that can be applied to any image, regardless of its source. This makes them less specifically tailored to the unique characteristics and requirements of AI-generated content.

\subsubsection{Tree-Ring}
Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023} is a pre-generation watermarking method for \acp{DM}. The watermark is encoded in Fourier space and is decoded by inverting the diffusion process to receive the noise vector which can be compared against the embedded signal.  \todo{Finish description}

\subsubsection{Stable Signature}
Stable Signature \cite{fernandezStableSignatureRooting2023b} is a watermarking approach specifically for \acp{LDM}. Stable signature embeds the watermark directly into the latent space of a model, rather than the pixel space, making it more resilient to post-processing transformations.



\section{Watermarking Optimisations and Enhancements}
\subsection{Perceptual Masking}
Perceptual masking exploits the characteristics of human vision by embedding watermarks into regions of an image where the changes will be less noticeable. For example, areas with high texture or edges rather than flat or uniform areas.

\section{Watermarking Challenges} 
\subsection{Attacks on Watermarks}


\section{Metrics for Evaluation}
Performance metrics such as \ac{MSE}, \ac{PSNR}, \ac{SSIM}, \ac{NCC} are commonly used to evaluate the imperceptibility and quality of watermarked images.
Performance metrics such as Mean Squared Error (MSE), Peak Signal-to-Noise-Ratio (PSNR), Structural Similarity Index Measure (SSIM), Normalised Cross-Correlation (NCC) are commonly used to evaluate the imperceptibility and quality of watermarked images.



\chapter{Methodology}
\label{cha:Methodology}

This chapter outlines the methodology for investigating and implementing digital watermarking within \ac{AI}-generated images, focusing on traceability and attribution. Given the prevalence and capabilities of modern generative techniques, \acp{LDM} are selected as the primary architecture. Specifically, Stable Diffusion \cite{rombachHighResolutionImageSynthesis2022} is chosen due to its open-source nature \cite{CompVisStablediffusion2024}, widespread use, and existing research on watermarking within its framework \cite{fernandezStableSignatureRooting2023b, zhaoRecipeWatermarkingDiffusion2023a, zhangAttackResilientImageWatermarking2024}. \acp{LDM} operate by denoising data in a compressed latent space, offering potentially more robust integration points for watermarking compared to pixel-space methods \cite{fernandezStableSignatureRooting2023b, rezaeiLaWaUsingLatent2024}. % Potentially detail the upcoming subsections of the methodology here.

\section{Watermarking Approach Selection}
\label{sec:WatermarkingApproachSelection}
The primary goal is to embed imperceptible watermarks that facilitate traceability and attribution. Based on the literature review, several approaches are viable, particularly those designed for or adaptable to \acp{DM}/\acp{LDM}.
Latent space modification techniques like Gaussian Shading \cite{yangGaussianShadingProvable2024}, Stable Signature \cite{fernandezStableSignatureRooting2023b}, Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023}, LaWa \cite{rezaeiLaWaUsingLatent2024}, ZoDiac \cite{zhangAttackResilientImageWatermarking2024}, and WMAdapter \cite{ciWMAdapterAddingWaterMark2024} propose embedding the watermark within the latent space during the image generation process (in-generation). This approach is specifically designed for AI image generation, as it integrates directly with the generative model's architecture and workflow. By embedding the watermark during the generation process itself, these methods can leverage the model's understanding of image structure and semantics, potentially leading to more robust watermark integration compared to post-generation methods.

The final selection of the watermarking technique is based on the following criteria:
\begin{enumerate}
    \item \textbf{Suitability for LDM Integration:} How readily can the technique be integrated into the Stable Diffusion architecture (e.g., VAE modification, U-Net fine-tuning)?
    \item \textbf{Robustness Potential:} Theoretical and empirical evidence from literature regarding resistance to common image manipulations (compression, noise, resizing) and potentially model-based attacks.
    \item \textbf{Capacity for Attribution Data:} Ability to embed a sufficient payload (e.g., unique identifier, timestamp) for traceability purposes \cite{jiangWatermarkbasedDetectionAttribution2024}.
    \item \textbf{Imperceptibility:} Maintaining high visual quality of the generated images (measured by PSNR, SSIM).
    \item \textbf{Implementation Feasibility:} Availability of reference implementations or clarity of the proposed algorithm within the project timeframe.
\end{enumerate}
Based on these criteria, Gaussian Shading \cite{yangGaussianShadingProvable2024} is selected as the watermarking technique for implementation. Its approach modifies the initial latent sampling process within the \ac{LDM} architecture, offering direct integration during generation (Criterion 1). A key advantage is its provably performance-lossless nature, meaning it does not require model fine-tuning and aims to preserve the original model's output quality and distribution (Criteria 1, 4, 5). The original paper reports high robustness against common distortions and good capacity (Criteria 2 \& 3), making it a strong candidate for embedding attribution data without compromising the user experience or requiring additional training resources. While other latent space methods like Stable Signature \cite{fernandezStableSignatureRooting2023b} or Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023} (originally designed primarily for 1-bit capacity, limiting its suitability for detailed attribution data under Criterion 3) also offer strong integration, Gaussian Shading's advantage for this project lies with it being performance-lossless without needing model fine-tuning or architectural changes (Criterion 5). This simplifies implementation and ensures the watermark minimally impacts the generative capabilities of the base Stable Diffusion model (Criterion 4), compared to approaches that might require adjustments to the VAE or U-Net.

\section{Implementation Plan}
\label{sec:ImplementationPlan}
The implementation will proceed as follows:
\begin{enumerate}
    \item \textbf{Environment Setup:} Configure a Python environment with necessary libraries including PyTorch, Diffusers (for Stable Diffusion), and potentially specific libraries for the chosen watermarking technique (e.g., LIEF if needed, libraries for image processing, evaluation metrics). Utilise available GPU resources for model training/fine-tuning and generation.
    \item \textbf{Model Acquisition:} Obtain a pre-trained Stable Diffusion model (e.g., v1.5, v2.1, or SDXL) as the base generative model.
    \item \textbf{Watermark Payload Definition:} Define the structure and content of the watermark message. For traceability and attribution, this could include a unique generation ID, a model identifier, and potentially a timestamp or user identifier (considering ethical implications). Error correction codes (e.g., BCH codes) will be incorporated to enhance robustness during extraction. \todo{Finalise watermark payload structure and size.}
    \item \textbf{Embedding Implementation:} Implement the watermark embedding mechanism based on the selected technique. This might involve:
        \begin{itemize}
            \item \textbf{Latent Sampling Modification:} Following the Gaussian Shading approach \cite{yangGaussianShadingProvable2024}, the initial latent variable $z_T$ sampling step will be modified. This involves: (i) Diffusing the watermark bits $s$ across the latent dimensions to get $s_d$. (ii) Randomising $s_d$ using a stream cipher (e.g., ChaCha20) with a secret key $K$ to get $m$. (iii) Sampling $z_T$ based on $m$ using the Gaussian quantile function and uniform random sampling (distribution-preserving sampling). This replaces the standard random sampling of $z_T \sim \mathcal{N}(0, I)$.
        \end{itemize}
    \item \textbf{Extraction Implementation:} Implement the corresponding watermark extraction algorithm. As per Gaussian Shading, this involves: (i) Using the standard VAE encoder to get $z_0'$ from the input image $X'$. (ii) Applying DDIM inversion to estimate the initial latent $z_T'$. (iii) Using the inverse sampling logic (Gaussian CDF) to extract the randomised watermark estimate $m'$ from $z_T'$. (iv) Decrypting $m'$ with the key $K$ to get $s_d'$. (v) Applying a reduction/voting mechanism to recover the final watermark estimate $s'$ from the diffused copies in $s_d'$.
    \item \textbf{Generation Pipeline:} Integrate the modified latent sampling process into the standard text-to-image generation pipeline of Stable Diffusion. The standard ODE sampler (e.g., DPMSolver) and VAE decoder are used without modification after the initial $z_T$ is sampled using the Gaussian Shading method.
\end{enumerate}

\section{Evaluation Framework}
\label{sec:EvaluationFramework}
To assess the effectiveness of the implemented watermarking scheme, a comprehensive evaluation will be conducted, focusing on the core requirements for digital watermarks: imperceptibility, robustness, and capacity.
\begin{itemize}
    \item \textbf{Dataset:} A standard dataset (e.g., MS-COCO captions, or a subset relevant to common AI generation prompts) will be used to generate a diverse set of watermarked images using various text prompts. A corresponding set of non-watermarked images will be generated for comparison. \todo{Specify dataset and number of images.}
    \item \textbf{Imperceptibility Assessment:}
        \begin{itemize}
            \item Quantitative metrics: \ac{PSNR} and \ac{SSIM} will be calculated between original (non-watermarked) and watermarked images. Higher values indicate better imperceptibility.
            \item Qualitative assessment: Visual inspection of watermarked images to identify any perceptual artifacts.
        \end{itemize}
    \item \textbf{Robustness Testing:} Watermarked images will be subjected to a range of common image processing attacks and distortions:
        \begin{itemize}
            \item Lossy Compression: JPEG compression at various quality factors (e.g., 90, 70, 50).
            \item Noise Addition: Gaussian noise, Salt-and-pepper noise.
            \item Geometric Transformations: Resizing, cropping, rotation.
            \item Filtering: Gaussian blur.
            % \item \textbf{Regeneration Attacks:} Evaluate robustness against regeneration using alternative VAEs and diffusion models (e.g., Stable Diffusion v1.4 if using v2.1 for generation). % Less critical for GS?
            \item \todo{Consider adding other relevant attacks from literature/benchmarks, e.g., specific adversarial attacks, combo distortions.}
        \end{itemize}
        The watermark will be extracted after each attack, and the Bit Error Rate (BER) between the original and extracted watermark payload will be calculated. Lower BER indicates higher robustness. Detection accuracy (whether the watermark presence is correctly identified) will also be measured.
    \item \textbf{Capacity Evaluation:} The size of the successfully embedded and extracted watermark payload (in bits) defines the capacity. This will be evaluated in conjunction with imperceptibility and robustness trade-offs.
    \item \textbf{False Positive Rate:} The watermark detector will be run on a set of non-watermarked images (both real-world images and AI-generated images from the base model without watermarking) to determine the rate at which it incorrectly detects a watermark. A low false positive rate is crucial for reliable attribution.
    \item \textbf{Computational Cost:} The overhead introduced by the watermarking process (embedding and extraction time) will be measured.
\end{itemize}
The results will be compared against baseline performance reported in the literature for the chosen or similar watermarking techniques.

\section{Tools and Environment}
\begin{itemize}
    \item \textbf{Programming Language:} Python 3.x
    \item \textbf{Core Libraries:} PyTorch, Hugging Face Diffusers, NumPy, Pandas, Matplotlib, Scikit-image, OpenCV.
    \item \textbf{Watermarking Specific Libraries:} No specific external libraries are strictly required for Gaussian Shading beyond the core ML stack, as the implementation primarily involves modifying the sampling logic within the existing framework using PyTorch and standard libraries (e.g., for stream ciphers if not built-in).
    \item \textbf{Hardware:} Access to GPU resources (e.g., NVIDIA GPU via local machine or cloud service like Google Colab) is required for efficient model execution and potential fine-tuning.
\end{itemize}

\section{Ethical Considerations}
The development and evaluation will adhere to ethical guidelines. Data used for generation prompts will be sourced appropriately. The watermark payload design will consider privacy implications, particularly if user identifiers are included. The potential for misuse of the watermarking technology itself (e.g., attempts to forge watermarks) is acknowledged, although developing countermeasures against such misuse is beyond the scope of this project's implementation phase. The focus remains on providing a reliable mechanism for traceability and attribution as mandated by emerging regulations \cite{houseExecutiveOrderSafe2023, RegulationEU20242024}.


\begin{figure}[htb]
\begin{center}
\includegraphics[height=3cm]{"./assets/UOY-Logo-Stacked-shield-Black"}
\end{center}
\caption{A figure containing UoY logo and its caption.}
\end{figure}

% \begin{table}[htb]
% \caption{ A table with its caption.}
% \begin{center}
% \begin{tabular}{|p{0.3\textwidth}|p{0.6\textwidth}|}
% \hline
% column A & column B \\\hline
% row 1 &
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque quis quam at nisi iaculis aliquet vel et quam. \\\hline
% row 2 &
% Aliquam erat volutpat. Nam at velit a risus faucibus aliquet. Aenean egestas vehicula mi, quis rhoncus sem facilisis in. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed lobortis lacus quis mauris rutrum auctor. \\\hline
% \end{tabular}
% \end{center}
% \end{table}


\chapter{Results}


\chapter{Conclusion and Future Work}
\label{cha:conclusion}


\appendix
\chapter{Some apendix}


\chapter{Another apendix}


\sloppy % relax spacing between words
\printbibliography

\end{document}