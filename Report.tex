\documentclass{UoYCSproject}

\setlength{\marginparwidth}{2cm}
\usepackage{todonotes} % to be removed, used for adding markers for references needed
\usepackage{marginfix}

\usepackage[nohyperlinks]{acronym} % addition for acronyms page
\usepackage{amsmath} % for equation environments
\usepackage{hyperref}

\addbibresource{bibliography.bib}
\author{Mischa}
\title{Identifying images generated by AI using watermarks}
\date{Version 3.0, 2020-November}
\supervisor{Dimitar Kazakov}
\BSc

\dedication{To all students everywhere}

\acknowledgements{
  I would like to thank my goldfish for all the help it gave me
  writing this document.
 
  As usual, my boss was an inspiring source of sagacious advice.
}

% More definitions & declarations in example.ldf

\begin{document}
\pagenumbering{roman}
\maketitle
\listoffigures
\listoftables
%\renewcommand*{\lstlistlistingname}{List of Listings}
%\lstlistoflistings

% List of Acronyms used
\chapter*{List of Acronyms}
\addcontentsline{toc}{chapter}{List of Acronyms}
\begin{acronym}[XXXXX]  % Use longest acronym width to suppress error
  \acro{AI}{Artificial Intelligence}
  \acro{GenAI}{Generative AI}
  \acro{GAIM}{Generative AI Image Model}
  \acro{GAII}{Generative AI Image}
  \acro{VAE}{Variational Autoencoder}
  \acro{GAN}{Generative Adversarial Network}
  \acro{DM}{Diffusion Model}
  \acro{LDM}{Latent Diffusion Model}
  \acro{RGB}{Red, Green, Blue}
  \acro{LSB}{Least Significant Bit}
  \acro{DCT}{Discrete Cosine Transform}
  \acro{DFT}{Discrete Fourier Transform}
  \acro{DWT}{Discrete Wavelet Transform}
  \acro{LPM}{Log-Polar Mapping}
  \acro{MSE}{Mean Squared Error}
  \acro{PSNR}{Peak Signal-to-Noise-Ratio}
  \acro{SSIM}{Structural Similarity Index Measure}
  \acro{NCC}{Normalised Cross-Correlation}
  \acro{BER}{Bit Error Rate}
  \acro{FID}{Fréchet Inception Distance}
\end{acronym}

\begin{summary}

\end{summary}


\begin{ethics}
% moved from methodology
The development and evaluation will adhere to ethical guidelines. Data used for generation prompts will be sourced appropriately. The watermark payload design will consider privacy implications, particularly if user identifiers are included. The potential for misuse of the watermarking technology itself (e.g., attempts to forge watermarks) is acknowledged, although developing countermeasures against such misuse is beyond the scope of this project's implementation phase. The focus remains on providing a reliable mechanism for traceability and attribution as mandated by emerging regulations \cite{houseExecutiveOrderSafe2023, RegulationEU20242024}.
\end{ethics}


\chapter{Introduction}
\label{cha:Introduction}
\section{Motivation and Problem Statement}
The proliferation of powerful \acp{GAIM} such as Stable Diffusion \cite{rombachHighResolutionImageSynthesis2022}, DALL-E \cite{rameshZeroShotTexttoImageGeneration2021}, and Midjourney mark a significant technological leap, unlocking new possibilities in fields like content creation, design prototyping, and automation. However, the ease with which these models can generate highly realistic synthetic media introduces critical ethical and practical challenges, including the spread of misinformation \cite{ferraraGenAIHumanityNefarious2024}, copyright infringement, and a general erosion of trust in digital content. Furthermore, current rulings by the US Copyright Office render purely \ac{AI}-generated images ineligible for copyright protection \cite{CopyrightRegistrationGuidance2023}.

In response, a clear consensus is forming around the need for establishing authenticity, attribution, and traceability mechanisms for synthetic media. Governmental bodies, including the White House through its 2023 Executive Order on \ac{AI} \cite{houseExecutiveOrderSafe2023} and the European Union via its 2024 AI Act \cite{RegulationEU20242024}, have mandated the use of techniques like invisible watermarking for synthetic content. Consequently, leading technology companies have begun integrating such solutions; notable examples include Google's SynthID \cite{IdentifyingAIgeneratedImages2024}, Microsoft's watermarking in Bing Image Creator \cite{BingPreviewRelease2023}, and Stability AI's watermarking methods for its models \cite{CompVisStablediffusion2024}.

The urgency for effective watermarking is further underscored by concerns surrounding the data used to train \acp{GAIM}. Artists have voiced concerns about the unauthorised use of their work \todo{artist concerns citation}, leading to \ac{AI}-generated images that mimic their unique styles without credit or compensation \todo{citation needed}. This has fueled the development of defensive techniques like Glaze \cite{shanGlazeProtectingArtists2023} and Nightshade \cite{shanNightshadePromptSpecificPoisoning2024}, which disrupt model training \cite{kurakinAdversarialExamplesPhysical2017}, highlighting the creative community's demand for control. While digital watermarking has a long history \cite{coxDigitalWatermarking2001}, its application to generative AI presents new challenges requires solutions that are integrated directly into the generation process. This project addresses the critical need for such a method, one that is imperceptible, robust against manipulation, capable of carrying attribution data without degrading image quality, and integrated within the generation process itself.

\section{Project Aims \& Scope}
The primary aim of this project is to investigate and evaluate the integration of digital watermarking within \ac{AI} image generation models to address the critical issues of \textbf{authenticity, attribution, traceability, and copyright protection}. To this end, the project will conduct a thorough evaluation of the Gaussian Shading watermarking algorithm, a state-of-the-art, training-free technique designed for \acp{LDM}.

To achieve this, the following objectives have been set:
\begin{enumerate}
    \item To conduct a focused review of modern watermarking techniques to identify the current state-of-the-art, justify the selection of Gaussian Shading, and position it within the field.
    \item To implement the Gaussian Shading algorithm within a standard \ac{GAIM} framework, specifically Stable Diffusion v2.1.
    \item To design and execute a comprehensive evaluation framework to test the watermark's imperceptibility and its robustness against a suite of common digital attacks.
    \item To analyse the results, assessing the crucial trade-offs between robustness, imperceptibility, and data capacity, and compare against published benchmarks to critically assess the performance and viability of Gaussian Shading as a practical watermarking solution.
\end{enumerate}

This project seeks to answer the following research questions:
\begin{itemize}
    \item To what extent can the Gaussian Shading method provide robust watermarking against a standard set of digital attacks, including compression, noise, and geometric transformations?
    \item What is the trade-off between watermark robustness and image quality? Can the method's "performance-lossless" claim be substantiated using metrics like \ac{PSNR}, \ac{SSIM}, and \ac{FID}?
    \item How does the performance of Gaussian Shading compare to other leading in-generation watermarking techniques.
\end{itemize}

The key contribution of this project is a rigorous and independent empirical evaluation of the Gaussian Shading watermarking method. By providing detailed performance data and a direct comparison to established benchmarks, this work offers valuable insights for the development of responsible and traceable \ac{GAII} generation systems.

\section{Dissertation Structure}
The remainder of this dissertation is organised as follows: Chapter 2 reviews the literature on generative models and watermarking, focusing on the state-of-the-art techniques that lead to the selection of Gaussian Shading. Chapter 3 details the methodology, system architecture, and the specific framework used for implementation and evaluation. Chapter 4 presents the results of the empirical evaluation, analysing the imperceptibility and robustness of the implemented solution. Finally, Chapter 5 concludes the dissertation, summarising the findings in relation to the research questions, discussing the study's limitations, and potential directions for future work.


\chapter{Literature Review}
\label{cha:Literature Review}
\section{Background}

\subsection{Generative Models in AI Image Generation}
Generative images have revolutionised the \ac{AI} field by enabling the creation of new data that closely resembles the training data. The three primary generative models used in \ac{AI} image generation being: \acp{GAN}, \acp{VAE}, and \acp{DM}.

\subsubsection{Variational Autoencoders (VAEs)}
\acp{VAE} were first defined in 2013 by Kingma et al. \cite{kingmaAutoEncodingVariationalBayes2022} and Rezende et al. \cite{rezendeStochasticBackpropagationApproximate2014}. \acp{VAE} are probabilistic generative models that learn a latent space representation of input data. They consist of an encoder and a decoder, which work together to reconstruct input data from a compressed latent space. Watermarking in \acp{VAE} could involve perturbing latent space to insert information \cite{guoFreqMarkInvisibleImage2024}.

\subsubsection{Generative Adversarial Networks}
\acp{GAN}, proposed by Goodfellow et al. \cite{goodfellowGenerativeAdversarialNetworks2014} in 2014. \acp{GAN} consist of two neural networks: A generator, and a discriminator. The generator takes an input of random noise and generates an image by reassembling the real data distribution; The discriminator seeks to differentiate between real and generated images. The competing nature of the model helps in improving the quality of images generated over time. However, images generated are sensitive to perturbations \cite{alfarraRobustnessQualityMeasures2022}. Therefore, adding a watermark could degrade the quality of the image.

\subsubsection{Diffusion Models}
\acp{DM} were first introduced in 2015 by Sohl-Dickstein et al. \cite{sohl-dicksteinDeepUnsupervisedLearning2015} and popularised in 2020 by Ho et al. \cite{hoDenoisingDiffusionProbabilistic2020}. Unlike \acp{GAN} and \acp{VAE}, \acp{DM} generate images by iteratively denoising a random noise pattern, producing high quality images with fine details. The challenge in watermarking \acp{DM} lies in ensuring the watermark does not interfere with the denoising process in order to preserve image quality. \acp{LDM} such as Stable Diffusion, extend the concept of diffusion by performing the denoising process in a latent space rather than directly in pixel space, allowing for more efficient training and inference, as well as improved image quality \cite{rombachHighResolutionImageSynthesis2022}. In \acp{LDM}, a \ac{VAE} is used to compress the image to into a lower-dimensional latent space, which is then denoised iteratively to generate the final image.

\subsection{Applications and Motivations for Watermarking in the AI Era}
The rise of \acp{GAIM} has created an urgent need for robust watermarking solutions to address several key challenges.

\subsubsection{Preventing Training Data Contamination and Model Collapse}
Many \acp{GAIM} are trained on large datasets scraped from the internet. A significant risk in this process is "model collapse", where models are inadvertently trained on their own synthetic outputs or those from other models. This can lead to a feedback loop that degrades the quality and diversity of generated content over time \cite{bohacekNepotisticallyTrainedGenerativeAI2023}. Watermarking \acp{GAII} can mitigate this by marking these synthetic outputs, allowing them to be filtered out of future training datasets, thereby preserving the integrity of the training data.

\subsubsection{Attribution, Traceability, and Content Authenticity}
Watermarks can act as digital fingerprints for \acp{GAII}, enabling robust attribution and traceability. Depending on the implementation, a watermark could encode information about the originating model, the user, or the time of generation. This is vital for accountability, particularly when addressing the use of copyrighted material in training data or the distribution of malicious content. Initiatives like the Coalition for Content Provenance and Authenticity (C2PA), backed by major technology firms, aim to standardise how this provenance information is embedded and read, creating a framework for digital content certification \cite{ContentCredentialsC2PA}. An effective watermarking scheme is therefore essential for copyright protection, verifying ownership, and ensuring the authenticity of digital media in an age of prolific AI generation \cite{jiangWatermarkbasedDetectionAttribution2024}.

\section{Digital Watermarking Techniques}
\subsection{Traditional Spatial Domain Watermarking}
Spatial domain watermarking involves embedding watermarks directly into the pixel values. These methods are straightforward, easy to implement, and computationally efficient. However, are typically less resistant to attacks such as compression and transformations.

\subsubsection{Least Significant Bit (LSB) Modification}
A common technique to embed a watermark information into randomly chosen pixels' \ac{LSB}. The \ac{LSB} is changed as to not affect the image quality as it contains less important information. However, it is trivial for an attacker to change all \ac{LSB} bits to 1 to modify the watermark. To address the problems with \ac{LSB} watermarking, improvements have been made. One such improvement embeds data not only to the \ac{LSB} but also higher planes. Moreover, a 2-3-3 embedding technique \cite{manjulaNovelHashBased2015} distributes the watermark across the \ac{RGB} channels of a pixel. This approach results in minimal perceptual distortion while achieving better embedding capacity and robustness.

\subsubsection{Patch-based or Block-Based techniques}
Proposed by Bender et al. \cite{benderTechniquesDataHiding1996} This method involves randomly picking $n$ pairs of image points $A,B$ where the image data in $A$ is darkened, while is brightened in $B$. This method offers decent robustness in exchange for capacity. \cite{saqibSpatialFrequencyDomain2017}

\subsection{Traditional Frequency (Transform) Domain Watermarking}
These techniques embed watermark information within the frequency domain of an image after a transformation. This approach spreads the watermark information throughout the image in ways that are less perceptible to the human eye and more resilient to common attacks compared to spatial methods.

\subsubsection{Discrete Cosine Transform}
\ac{DCT} watermarking embeds watermark information into an images frequency coefficients after transforming it from the spatial to the frequency domain. This leverages energy compaction, where the majority of an image's visual information is represented by lower-frequency coefficients, while higher-frequency coefficients capture finer image details. A common approach is block-based \ac{DCT}, where the image is divided into smaller non-overlapping blocks, \ac{DCT} is then applied to each block. Mid-frequency coefficients are typically chosen, balancing imperceptibility and robustness. Modifying low-frequency coefficients can lead to more noticeable distortions, while high-frequency coefficients are more susceptible to compression and noise attacks. Block-based \ac{DCT} is particularly suitable for JPEG compression, a prevalent image compression technique which is also block-based \cite{wallaceJPEGStillPicture1991}. By embedding watermarks in \ac{DCT} coefficients compatible with JPEG's compression algorithm, the watermark can survive compression without significant degradation \cite{borsImageWatermarkingUsing1996}. Alternatively, global \ac{DCT} applies the transformation to the entire image rather than individual blocks. This offers greater robustness against attacks, but is more computationally intensive and less compatible with block-based compression techniques such as JPEG.

The robustness of \ac{DCT}-based watermarking comes from the ability to embed data in perceptually significant regions of an image, therefore being less likely to be removed by common image processing operations. However, \ac{DCT} based watermarking methods struggle with maintaining robustness against geometric attacks such as scaling and rotation due to inherently not accounting for spatial transformations \cite{fazliRobustImageWatermarking2016}. From this hybrid techniques combining \ac{DCT} with other transformations have arisen \cite{abdulrahmanNovelHybridDCT2019}.

\subsubsection{Discrete Fourier Transform}
Similar to \ac{DCT} watermarking, \ac{DFT} embeds watermark information into an images frequency domain by transforming it from the spatial domain to the frequency domain, but using the \ac{DFT} which decomposes an image into sinusoidal components of varying frequencies, represented as complex-valued coefficients corresponding to magnitude and phase. These coefficients describe the global frequency characteristics of the image, making \ac{DFT}-based watermarking inherently robust against various image processing operations and certain geometric transformations.

\ac{LPM} transforms the image into log-polar coordinates before applying \ac{DFT}. This mapping converts scaling and rotation into linear translations in the frequency domain, enabling efficient watermark extraction after significant geometric transformations \cite{zhengRSTinvariantDigitalImage2003}.

\subsubsection{Discrete Wavelet Transform}
A \ac{DWT} is any wavelet transform that decomposes a signal into wavelets, offering local analysis in both the time and frequency domains. Unlike \ac{DFT}, which analyses global frequency count, and \ac{DCT} which can operate globally or block-based, \ac{DWT} inherently supports multi-resolution analysis by examining signals at different scales. This dual localisation makes \ac{DWT} particularly effective for image watermarking, as it can capture coarse and fine image details simultaneously.

\subsubsection{Applications to AI-Generated Images}
Traditional frequency domain methods (DCT, DWT) represent conventional image watermarking approaches that operate in the frequency domain. While well-established and widely used for natural images, these techniques are generic post-processing methods that don't specifically leverage the properties of the AI generation processes. Their application remains relatively unexplored in recent LDM-specific literature compared to direct latent modification approaches that are purposefully designed for AI-generated content.

\subsection{Post-Generation Methods}
Methods like HiDDeN \cite{zhuHiDDeNHidingData2018} and FreqMark \cite{guoFreqMarkInvisibleImage2024} demonstrate impressive results, they are designed as post-generation solutions that can be applied to any image, regardless of its source. This makes them less specifically tailored to the unique characteristics and requirements of AI-generated content.
\subsubsection{HiDDeN}
One promising advancement in watermarking is the HiDDeN framework \cite{zhuHiDDeNHidingData2018}. HiDDeN leverages the sensitivity of deep neural networks to small perturbations in input images to encode information, making it a robust solution for watermarking.

The HiDDeN framework comprises three main components: an encoder, a decoder, and an adversary network. The encoder receives an image and a message string, outputting an encoded image that incorporates the watermark. The decoder attempts to reconstruct the original message from the encoded image, while the adversary network predicts whether a given image contains an encoded watermark, providing adversarial loss to enhance the quality of the encoded images.

The adversarial training enhances the watermark's resilience against numerous attacks. The deep leaning approach allows for a more flexible watermark embedding,

\subsection{Modern Watermarking for AI-Generated Images}
Recent advancements have focused on integrating watermarks directly into the generative process of \acp{DM} and \acp{LDM}, offering greater robustness and imperceptibility compared to traditional post-processing methods.

\subsubsection{Tree-Ring}
Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023} is a notable in-generation technique for \acp{DM}. It operates by embedding a watermark signal into the initial noise vector ($z_{T}$) before the diffusion process begins. The core idea is that the deterministic nature of DDIM inversion allows for the retrieval of the initial noise vector from the final generated image. By comparing the recovered noise with the known watermark signal, the presence of the watermark can be detected. While effective for detection, its original design is primarily for a 1-bit watermark, which limits its viability for use other than binary watermark detection.

\subsubsection{Stable Signature}
Stable Signature \cite{fernandezStableSignatureRooting2023b} is a watermarking approach designed specifically for \acp{LDM}. Instead of modifying the initial noise, this method fine-tunes the model's autoencoder (VAE). The VAE's decoder is trained to embed a specific watermark pattern into the generated image's pixel space while the encoder is trained to be robust to its presence. By embedding the watermark directly into the model's architecture, Stable Signature aims to create a watermark that is deeply integrated with the image's content and thus more resilient to post-processing attacks.

\subsubsection{Gaussian Shading}
Gaussian Shading \cite{yangGaussianShadingProvable2024} is a training-free, performance-lossless watermarking technique for \acp{LDM}. Unlike methods that require model fine-tuning, Gaussian Shading is a plug-and-play solution that modifies the initial latent sampling step. The watermark is mapped to a latent representation that follows a standard Gaussian distribution, making it statistically indistinguishable from a non-watermarked latent vector. This ensures that the watermarking process does not degrade the performance of the generative model. The watermark is embedded by diffusing the bits across the latent dimensions and then using a distribution-preserving sampling method. Extraction is achieved through DDIM inversion to retrieve an estimate of the initial latent, from which the watermark can be recovered. The authors provide theoretical proof of its performance-lossless nature and demonstrate high robustness against common attacks, outperforming many existing methods.

\subsubsection{Comparative Analysis}
To provide a clear overview, Table \ref{tab:watermarking_comparison} compares the different watermarking techniques discussed.

\begin{table}[htb]
\caption{Comparison of Digital Watermarking Techniques.}
\label{tab:watermarking_comparison}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Technique} & \textbf{Domain} & \textbf{Robustness} & \textbf{Imperceptibility} & \textbf{Capacity} & \textbf{Training-Free} \\ \hline
LSB & Spatial & Low & High & High & Yes \\ \hline
DCT & Frequency & Medium & Medium & Medium & Yes \\ \hline
DWT & Frequency & High & High & Medium & Yes \\ \hline
Tree-Ring & Latent (DM) & High & High & Low (1-bit) & Yes \\ \hline
Stable Signature & Latent (LDM) & High & High & High & No \\ \hline
Gaussian Shading & Latent (LDM) & High & High (Provably Lossless) & High & Yes \\ \hline
\end{tabular}%
}
\end{center}
\end{table}

\section{Watermarking Optimisations and Enhancements}
\subsection{Perceptual Masking}
Perceptual masking exploits the characteristics of human vision by embedding watermarks into regions of an image where the changes will be less noticeable. For example, areas with high texture or edges rather than flat or uniform areas.

\section{Watermarking Challenges and Evaluation}
A successful watermarking scheme must be robust against a variety of attacks designed to remove or degrade the embedded information. Furthermore, its performance must be quantifiable using standard evaluation metrics.

\subsection{Attacks on Watermarks}
Watermarks are susceptible to a wide range of attacks, which can be broadly categorised as follows:
\textbf{Removal Attacks:} These are designed to completely eliminate the watermark signal from the image. This can include denoising filters or adversarial attacks specifically trained to target and erase the watermark.
\textbf{Geometric Attacks:} These attacks alter the geometry of the image, which can desynchronise the detector. Common examples include rotation, scaling, cropping, and translation.
\textbf{Signal Processing Attacks:} These are common image manipulations that can unintentionally degrade or destroy the watermark. This category includes lossy compression (e.g., JPEG), noise addition (e.g., Gaussian noise), and filtering (e.g., blurring).

A robust watermarking system must be able to withstand a combination of these attacks to be considered effective in real-world scenarios.

\subsection{Metrics for Evaluation}
\todo{maybe mention MSE, NCC}
To objectively assess the performance of a watermarking technique, a set of standard metrics is used to measure three key properties:
\begin{itemize}
    \item \textbf{Imperceptibility:} This measures the visual distortion introduced by the watermark. It is commonly quantified using metrics like \ac{PSNR} and \ac{SSIM}. Higher values indicate that the watermarked image is visually closer to the original, meaning the watermark is less perceptible.
    \item \textbf{Robustness:} This measures the watermark's ability to survive attacks. It is typically evaluated by calculating the Bit Error Rate (BER) between the original and extracted watermark message after an attack has been applied. A lower BER indicates higher robustness.
    \item \textbf{Capacity:} This refers to the amount of information (in bits) that can be embedded within the watermark. There is often a trade-off between capacity, robustness, and imperceptibility.
\end{itemize}
These metrics provide the foundation for the experimental evaluation framework described in the next chapter.

\chapter{Methodology}
\label{cha:Methodology}

This chapter details the approach taken to investigate, implement, and evaluate the Gaussian Shading watermarking technique for \acp{GAII}. It covers the research strategy, the selection of the specific watermarking technique from available literature, a detailed experimental design, and the comprehensive framework for evaluation. The methodology will provide a rigorous assessment that enables direct comparison with state-of-the-art techniques while operating within the feasibility constraints of an undergraduate project.

\section{Research Approach}
\label{sec:ResearchApproach}

This study employs a \textbf{quantitative empirical evaluation} methodology to assess the performance of Gaussian Shading watermarking. The approach is aligned with established benchmarks in the watermarking literature, specifically following the evaluation protocols used by Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023} and Stable Signature \cite{fernandezStableSignatureRooting2023b}, enabling direct comparison without requiring re-implementation of competing methods.

The selection of \acp{LDM} as the primary architecture is justified by their current dominance in high quality image generation and the availability of open-source implementations \todo{cite?}. Specifically, Stable Diffusion \cite{rombachHighResolutionImageSynthesis2022} is chosen due to its open-source nature \cite{CompVisStablediffusion2024}, widespread adoption in research, and existing watermarking literature within its framework \cite{fernandezStableSignatureRooting2023b, zhaoRecipeWatermarkingDiffusion2023a, zhangAttackResilientImageWatermarking2024}.

\section{Watermarking Technique Selection}
\label{sec:WatermarkingTechniqueSelection}

The primary goal is to embed imperceptible watermarks that facilitate traceability and attribution. Based on the literature review, several approaches are viable, particularly those designed for or adaptable to \acp{LDM}. Latent space modification techniques like Stable Signature \cite{fernandezStableSignatureRooting2023b}, Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023}, LaWa \cite{rezaeiLaWaUsingLatent2024}, ZoDiac \cite{zhangAttackResilientImageWatermarking2024}, and WMAdapter \cite{ciWMAdapterAddingWaterMark2024} propose embedding the watermark within the latent space during the image generation process. This approach is specifically designed for AI image generation, as it integrates directly with the generative model's architecture and workflow.

The final selection of the watermarking technique was based on the following criteria:
\begin{enumerate}
    \item \textbf{Suitability for LDM Integration:} How readily the technique can be integrated into the Stable Diffusion architecture.
    \item \textbf{Robustness Potential:} Theoretical and empirical evidence from literature regarding resistance to common image manipulations.
    \item \textbf{Capacity for Attribution Data:} Ability to embed a sufficient payload for traceability purposes \cite{jiangWatermarkbasedDetectionAttribution2024}.
    \item \textbf{Imperceptibility:} Maintaining high visual quality of the generated images.
    \item \textbf{Implementation Feasibility:} Availability of reference implementations or clarity of the proposed algorithm within the project timeframe.
\end{enumerate}

Based on these criteria, Gaussian Shading \cite{yangGaussianShadingProvable2024} was selected. Its approach modifies the initial latent sampling process, offering direct integration (Criterion 1). A key advantage is its provably performance-lossless nature, meaning it does not require model fine-tuning and aims to preserve the original model's output quality (Criteria 1, 4, 5). The original paper reports high robustness and good capacity (Criteria 2 \& 3), making it a strong candidate. While other latent space methods like Stable Signature \cite{fernandezStableSignatureRooting2023b} or Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023} (originally designed primarily for 1-bit capacity, limiting its suitability for detailed attribution data under Criterion 3) also offer strong integration, Gaussian Shading's advantage for this project lies with it being performance-lossless without needing model fine-tuning or architectural changes (Criterion 5). This simplifies implementation and ensures the watermark minimally impacts the generative capabilities of the base Stable Diffusion model (Criterion 4), compared to approaches that might require adjustments to the VAE or U-Net.


\section{Experimental Design}
\label{sec:ExperimentalDesign}

\subsection{Standard Evaluation Protocol}
\label{subsec:StandardEvaluationProtocol}

To ensure reproducible and comparable results, the evaluation follows a standardised protocol:

\begin{itemize}
    \item \textbf{Generative Model:} Stable Diffusion v2.1-base (\texttt{stabilityai/stable-diffusion-2-1-base})
    \item \textbf{Image Resolution:} 512×512 pixels (standard for SD v2.1)
    \item \textbf{Guidance Scale:} 7.5 (classifier-free guidance strength)
    \item \textbf{Inference Steps:} 50 (using DPMSolver++ scheduler)
    \item \textbf{Precision:} Float16 for memory efficiency
    \item \textbf{Batch Size:} 1 (for reproducibility and memory constraints)
\end{itemize}

These parameters align with established benchmarks and ensure compatibility with the RTX 2070 Super GPU constraints (8GB VRAM) while maintaining generation quality.

\subsection{Dataset Selection and Justification}
\label{subsec:DatasetSelection}

The evaluation employs a two-tier dataset strategy to balance comprehensive evaluation with computational feasibility:

\textbf{Primary Dataset - COCO-5K Subset:}
\begin{itemize}
    \item \textbf{Source:} 1000 prompts from MS-COCO validation set
    \item \textbf{Purpose:} \ac{FID} calculation and baseline quality assessment
    \item \textbf{Rationale:} Enables direct comparison with Tree-Ring's published \ac{FID} results
    \item \textbf{Format:} Using identical \texttt{meta\_data.json} structure as Tree-Ring evaluation
\end{itemize}

\textbf{Secondary Dataset - Diverse Prompt Evaluation:}
\begin{itemize}
    \item \textbf{Source:} 200 prompts from \texttt{Gustavosta/Stable-Diffusion-Prompts}
    \item \textbf{Purpose:} Robustness evaluation across attack configurations
    \item \textbf{Selection Criteria:} Diverse prompt categories (portraits, landscapes, objects, abstract)
    \item \textbf{Rationale:} Provides sufficient statistical power while remaining computationally feasible
\end{itemize}

The reduced dataset size (200 vs. 1000+ in some studies) is justified by the undergraduate project scope and hardware constraints, while maintaining statistical validity through proper experimental design.

\subsection{Watermark Configuration}
\label{subsec:WatermarkConfiguration}

The watermark implementation follows the Gaussian Shading specification:
\begin{itemize}
    \item \textbf{Capacity:} 256 bits
    \item \textbf{Payload Content:} For empirical testing, the payload is a random binary string for each generation to facilitate robust Bit Error Rate (BER) analysis. In a practical application, this payload would be structured to contain attribution data such as a unique generation ID, a model identifier, or a timestamp.
    \item \textbf{Error Correction:} To enhance robustness in a real-world scenario, the payload would incorporate error correction codes (e.g., BCH codes) before the embedding process.
    \item \textbf{Encryption:} ChaCha20 stream cipher with a random key/nonce per image.
    \item \textbf{Diffusion Parameters:} \texttt{channel\_copy = 1}, \texttt{hw\_copy = 8}, as per the standard configuration in \cite{yangGaussianShadingProvable2024}.
    \item \textbf{False Positive Rate:} Target detection threshold set for an FPR of $10^{-6}$.
\end{itemize}

\section{System Architecture}
\label{sec:SystemArchitecture}

The Gaussian Shading watermarking system operates through two primary pipelines:

\textbf{Watermark Embedding Pipeline:}
\begin{enumerate}
    \item Text prompt processing via CLIP text encoder.
    \item The watermark message $s$ is diffused across the latent dimensions to get $s_{d}$.
    \item The diffused watermark $s_{d}$ is randomised using a ChaCha20 stream cipher with a secret key $K$ to get $m$.
    \item The initial latent $z_{T}$ is sampled based on $m$ using the Gaussian quantile function and uniform random sampling (distribution-preserving sampling), replacing the standard random sampling of $z_{T} \sim \mathcal{N}(0, I)$.
    \item The standard diffusion denoising process is applied to $z_{T}$ for 50 steps.
    \item The VAE decoder is used to decode the denoised latent $z_{T}$ to the watermarked image.
\end{enumerate}

\textbf{Watermark Extraction Pipeline:}
\begin{enumerate}
    \item VAE encoding of suspected watermarked image to get an initial latent representation $z_{0}'$.
    \item DDIM inversion is applied to estimate the initial latent noise $z_{T}'$.
    \item The inverse sampling logic (Gaussian CDF) is used to extract the randomised watermark estimate $m'$ from $z_{T}'$.
    \item The message $m'$ is decrypted with the key $K$ to get the diffused watermark estimate $s_{d}'$.
    \item A reduction/voting mechanism is applied to recover the final watermark estimate $s'$ from the diffused copies in $s_{d}'$.
    \item Binary classification (watermarked vs. non-watermarked) based on the recovered bits.
\end{enumerate}

\section{Implementation Details}
\label{sec:ImplementationDetails}

\subsection{Technical Environment}
\label{subsec:TechnicalEnvironment}

\begin{itemize}
    \item \textbf{Hardware:} NVIDIA RTX 2070 Super (8GB VRAM)
    \item \textbf{Software Stack:} Python 3.8, PyTorch 1.13+, Diffusers 0.11.1
    \item \textbf{Dependencies:} transformers, accelerate, scipy, PIL, numpy
    \item \textbf{Cryptography:} pycryptodome for ChaCha20 implementation
\end{itemize}

The RTX 2070 Super constraint necessitates memory-efficient implementation strategies, including float16 precision and sequential processing rather than batch generation.

\subsection{Gaussian Shading Implementation}
\label{subsec:GaussianShadingImplementation}

The implementation follows the algorithm specification from \cite{yangGaussianShadingProvable2024}:

\textbf{Distribution-Preserving Sampling:}
\begin{equation}
z_{T} \sim \mathcal{N}(0, I) \text{ (standard)}
\end{equation}
\begin{equation}
z_{T}^{(w)} = \Phi^{-1}(U \cdot \Phi(z_{T}) + (1-U) \cdot \Phi(z_{T}^{(ref)})) \text{ (watermarked)}
\end{equation}

where $\Phi$ is the standard normal CDF, $U$ is uniform random, and $z_{T}^{(ref)}$ encodes the encrypted watermark bits.

\textbf{DDIM Inversion for Extraction:}
The extraction process uses deterministic DDIM inversion with 50 steps to estimate the initial latent, enabling watermark recovery through the inverse sampling process.

\section{Evaluation Framework}
\label{sec:EvaluationFramework}

\subsection{Attack Suite Configuration}
\label{subsec:AttackSuiteConfiguration}

The robustness evaluation employs a comprehensive attack suite designed to test common image processing operations:

\begin{table}[htb]
\caption{Attack Configuration Parameters}
\label{tab:attack_parameters}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Attack Type} & \textbf{Parameters} & \textbf{Configurations} \\
\hline
JPEG Compression & Quality Factor & 90, 75, 50, 25 \\
\hline
Gaussian Noise & Standard Deviation & 0.01, 0.03, 0.05 \\
\hline
Gaussian Blur & Kernel Radius & 2, 4 \\
\hline
Brightness Adjustment & Multiplication Factor & 0.5, 2.0 \\
\hline
Random Crop & Retention Ratio & 0.8 \\
\hline
\end{tabular}
\end{center}
\end{table}

This configuration yields 12 attack scenarios, each evaluated on 200 images, totaling 2400 robustness experiments. The parameter ranges are selected to align with previous watermarking studies while covering mild to severe attack intensities.

\subsection{Evaluation Metrics}
\label{subsec:EvaluationMetrics}

\textbf{Imperceptibility Metrics:}
\begin{itemize}
    \item \textbf{PSNR:} Peak Signal-to-Noise Ratio measuring pixel-level similarity.
    \item \textbf{SSIM:} Structural Similarity Index assessing perceptual quality.
    \item \textbf{FID:} Fréchet Inception Distance evaluating distribution preservation.
    \item \textbf{CLIP Score:} Semantic similarity between prompt and generated image.
\end{itemize}

\textbf{Robustness Metrics:}
\begin{itemize}
    \item \textbf{Bit Error Rate (BER):} Primary robustness measure.
    \item \textbf{True Positive Rate (TPR):} Detection accuracy at fixed FPR.
    \item \textbf{Detection Accuracy:} Binary classification performance.
\end{itemize}

\subsection{Comparative Analysis Strategy}
\label{subsec:ComparativeAnalysisStrategy}

Rather than implementing competing methods, the evaluation strategy employs \textbf{benchmark extraction} from published literature:

\begin{enumerate}
    \item \textbf{Literature Analysis:} Extract performance metrics from Tree-Ring \cite{wenTreeRingsWatermarksInvisible2023} and Stable Signature \cite{fernandezStableSignatureRooting2023b} papers.
    \item \textbf{Standardised Comparison:} Align evaluation datasets and attack parameters with published benchmarks.
    \item \textbf{Statistical Analysis:} Report confidence intervals and significance tests where applicable.
    \item \textbf{Performance Tables:} Create comparative tables showing relative performance across methods.
\end{enumerate}

This approach enables rigorous comparison while remaining feasible within project constraints.

\subsection{Statistical Rigor and Reproducibility}
\label{subsec:StatisticalRigor}

To ensure scientific validity:

\begin{itemize}
    \item \textbf{Fixed Seeds:} Deterministic random number generation for reproducibility.
    \item \textbf{Sample Sizes:} Sufficient for statistical power (n=200 per attack, n=1000 for FID).
    \item \textbf{Confidence Intervals:} 95\% CI reported for key metrics.
    \item \textbf{Version Control:} All code and hyperparameters documented.
\end{itemize}

\subsection{Success Criteria and Thresholds}
\label{subsec:SuccessCriteria}

The evaluation establishes clear success criteria to assess the viability of Gaussian Shading:

\textbf{Imperceptibility Thresholds:}
\begin{itemize}
    \item \textbf{PSNR:} > 40 dB (high perceptual quality)
    \item \textbf{SSIM:} > 0.95 (strong structural similarity)
    \item \textbf{FID:} < 5.0 (distribution preservation)
\end{itemize}

\textbf{Robustness Thresholds:}
\begin{itemize}
    \item \textbf{BER:} < 0.1 for mild attacks (JPEG QF $\geq$ 75)
    \item \textbf{BER:} < 0.25 for moderate attacks (JPEG QF $\geq$ 50)
    \item \textbf{TPR:} > 0.9 at FPR = $10^{-6}$
\end{itemize}

These thresholds are derived from watermarking literature and provide objective criteria for evaluating the performance-lossless claims of Gaussian Shading.

The methodology balances comprehensive evaluation with practical constraints, ensuring that results are both scientifically valid and directly comparable to existing literature while remaining achievable within undergraduate project scope and hardware limitations.


\chapter{Results}


\chapter{Conclusion}
\label{cha:conclusion}
% Potentially needs to be rethought to be more concise


%This chapter summarises the findings of the project, revisits the research questions, and discusses the implications of the results. It also acknowledges the limitations of the study and proposes potential avenues for future research.

\section{Summary of Findings}
%This section will summarise the key results from Chapter 4, directly answering the research questions posed in the introduction. It will provide a concise overview of the measured performance of Gaussian Shading in terms of imperceptibility and robustness.

\section{Achievement of Objectives}
%This section will reflect on the project objectives outlined in Chapter 1, detailing how each was met through the research, implementation, and evaluation process.

\section{Limitations of the Study}
%This section will critically assess the limitations of the project. This may include the scope of the attacks tested, the specific version of the generative model used, or the constraints of the evaluation dataset.

\section{Future Work}
%Building on the findings and limitations, this section will propose concrete directions for future research. This could include testing against more advanced adversarial attacks, evaluating the technique on different generative architectures (e.g., SDXL), or exploring methods to increase payload capacity without sacrificing robustness.

\section{Final Remarks}
%This section will offer concluding thoughts on the importance of robust watermarking for the future of responsible AI and the potential role of techniques like Gaussian Shading in achieving this goal.


\appendix
\chapter{Some appendix}


\chapter{Another appendix}


\sloppy % relax spacing between words
\printbibliography

\end{document}